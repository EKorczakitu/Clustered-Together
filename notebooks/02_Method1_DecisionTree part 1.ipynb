{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# From-scratch WEIGHTED Regression Tree (NumPy only)\n",
    "# + metrics\n",
    "# + memory-light cost-complexity pruning (α) with CV helpers\n",
    "# =========================\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# -------- Metrics --------\n",
    "def wmae(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return (np.abs(y - yhat) * w).sum() / w.sum()\n",
    "\n",
    "def wrmse(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return np.sqrt(((y - yhat)**2 * w).sum() / w.sum())\n",
    "\n",
    "def weighted_r2(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    ybar = (y * w).sum() / w.sum()\n",
    "    ss_res = ((y - yhat)**2 * w).sum()\n",
    "    ss_tot = ((y - ybar)**2 * w).sum()\n",
    "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "def poisson_deviance(y_counts, exposure, yhat_rate):\n",
    "    \"\"\"\n",
    "    Exposure-weighted Poisson deviance for counts.\n",
    "    Safe for zeros: clips lam = rate*exposure and ignores impossible rows (exp<=0 & y>0).\n",
    "    \"\"\"\n",
    "    y   = np.asarray(y_counts,  float)\n",
    "    exp = np.asarray(exposure,  float)\n",
    "    lam = np.asarray(yhat_rate, float) * exp\n",
    "\n",
    "    # guard impossible rows (positive counts with zero/neg exposure)\n",
    "    bad = (exp <= 0) & (y > 0)\n",
    "    if np.any(bad):\n",
    "        y   = y[~bad]\n",
    "        exp = exp[~bad]\n",
    "        lam = lam[~bad]\n",
    "\n",
    "    # clip lambda away from zero\n",
    "    lam = np.clip(lam, 1e-12, None)\n",
    "\n",
    "    # safe deviance\n",
    "    term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
    "    return 2.0 * np.nansum(term)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def wmean(y, w):\n",
    "    sw = w.sum()\n",
    "    return (y * w).sum() / sw if sw > 0 else 0.0\n",
    "\n",
    "def leaf_sse_from_stats(sw, swy, swy2):\n",
    "    # SSE = sum w(y - mean)^2 = sum(w*y^2) - (sum(w*y))^2 / sum(w)\n",
    "    return float(swy2 - (swy * swy) / sw) if sw > 0 else 0.0\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    feature: Optional[int] = None\n",
    "    threshold: Optional[float] = None\n",
    "    left: Optional[\"Node\"] = None\n",
    "    right: Optional[\"Node\"] = None\n",
    "    value: Optional[float] = None   # leaf prediction (weighted mean)\n",
    "\n",
    "    # sufficient stats for THIS NODE'S samples (fixed after fit)\n",
    "    sw: float = 0.0                  # sum of weights\n",
    "    swy: float = 0.0                 # sum of w*y\n",
    "    swy2: float = 0.0                # sum of w*y^2\n",
    "\n",
    "    # subtree stats (updated during pruning)\n",
    "    subtree_sse: float = 0.0\n",
    "    subtree_leaves: int = 1\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeRegressorScratch:\n",
    "    \"\"\"\n",
    "    Exposure-weighted regression tree for claim rate.\n",
    "    - Split criterion: minimize weighted SSE.\n",
    "    - Leaf value: exposure-weighted mean.\n",
    "    - Pre-pruning: max_depth, min_leaf_weight (exposure units).\n",
    "    Uses only NumPy for training/prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth: Optional[int] = None, min_leaf_weight: float = 5.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_weight = float(min_leaf_weight)\n",
    "        self.root: Optional[Node] = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, w: np.ndarray):\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y, float)\n",
    "        w = np.asarray(w, float)\n",
    "        idx = np.arange(X.shape[0], dtype=int)\n",
    "        self.root = self._build_tree(X, y, w, idx, depth=0)\n",
    "        _update_subtree_stats(self.root)  # init subtree stats        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, float)\n",
    "        return np.array([self._traverse(x, self.root) for x in X], float)\n",
    "\n",
    "    # ----- internal: build tree -----\n",
    "    def _build_tree(self, X, y, w, idx, depth) -> Node:\n",
    "        sw = w[idx].sum()\n",
    "        swy = (y[idx] * w[idx]).sum()\n",
    "        swy2 = ((y[idx] ** 2) * w[idx]).sum()\n",
    "\n",
    "        # stopping rules\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (sw < 2 * self.min_leaf_weight) or \\\n",
    "           np.allclose(y[idx], y[idx][0]):\n",
    "            return Node(value=(swy / sw if sw > 0 else 0.0), sw=sw, swy=swy, swy2=swy2,\n",
    "                        subtree_sse=leaf_sse_from_stats(sw, swy, swy2), subtree_leaves=1)\n",
    "\n",
    "        feat, thr, L, R = self._best_split(X, y, w, idx)\n",
    "        if feat is None:\n",
    "            return Node(value=(swy / sw if sw > 0 else 0.0), sw=sw, swy=swy, swy2=swy2,\n",
    "                        subtree_sse=leaf_sse_from_stats(sw, swy, swy2), subtree_leaves=1)\n",
    "\n",
    "        left  = self._build_tree(X, y, w, L, depth+1)\n",
    "        right = self._build_tree(X, y, w, R, depth+1)\n",
    "        return Node(feature=feat, threshold=thr, left=left, right=right,\n",
    "                    value=None, sw=sw, swy=swy, swy2=swy2)\n",
    "\n",
    "    # ----- internal: best split -----\n",
    "    def _best_split(self, X, y, w, idx) -> Tuple[Optional[int], Optional[float], Optional[np.ndarray], Optional[np.ndarray]]:     \n",
    "        best = (None, None, None, None, np.inf)\n",
    "        n, d = X.shape\n",
    "        for j in range(d):\n",
    "            xj = X[idx, j]\n",
    "            uniq = np.unique(xj)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "            # thresholds: midpoints; for one-hot 0/1, just 0.5\n",
    "            if uniq.size == 2 and uniq.min() == 0.0 and uniq.max() == 1.0:\n",
    "                candidates = [0.5]\n",
    "            else:\n",
    "                u = np.unique(np.sort(xj))\n",
    "                candidates = (u[:-1] + u[1:]) / 2.0\n",
    "\n",
    "            for t in candidates:\n",
    "                Lmask = xj <= t\n",
    "                if not Lmask.any() or Lmask.all():\n",
    "                    continue\n",
    "                L = idx[Lmask]; R = idx[~Lmask]\n",
    "\n",
    "                swL = w[L].sum(); swR = w[R].sum()\n",
    "                if swL < self.min_leaf_weight or swR < self.min_leaf_weight:\n",
    "                    continue\n",
    "\n",
    "                swyL = (y[L] * w[L]).sum(); swyR = (y[R] * w[R]).sum()\n",
    "                swy2L = ((y[L]**2) * w[L]).sum(); swy2R = ((y[R]**2) * w[R]).sum()\n",
    "                sseL = leaf_sse_from_stats(swL, swyL, swy2L)\n",
    "                sseR = leaf_sse_from_stats(swR, swyR, swy2R)\n",
    "                sse  = sseL + sseR\n",
    "                if sse < best[4]:\n",
    "                    best = (j, t, L, R, sse)\n",
    "\n",
    "        return best[0], best[1], best[2], best[3]\n",
    "\n",
    "    # ----- internal: predict one -----\n",
    "    def _traverse(self, x, node: Node) -> float:\n",
    "        while node.value is None:\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n",
    "\n",
    "# ---- subtree stat maintenance & pruning (streaming, memory-light) ----\n",
    "def _update_subtree_stats(node: Node) -> Tuple[float, int]:\n",
    "    if node.value is not None:\n",
    "        node.subtree_sse = leaf_sse_from_stats(node.sw, node.swy, node.swy2)\n",
    "        node.subtree_leaves = 1\n",
    "        return node.subtree_sse, 1\n",
    "    sL, lL = _update_subtree_stats(node.left)\n",
    "    sR, lR = _update_subtree_stats(node.right)\n",
    "    node.subtree_sse = sL + sR\n",
    "    node.subtree_leaves = lL + lR\n",
    "    return node.subtree_sse, node.subtree_leaves\n",
    "\n",
    "def _alpha_of(node: Node) -> float:\n",
    "    if node.value is not None:\n",
    "        return np.inf\n",
    "    sse_leaf = leaf_sse_from_stats(node.sw, node.swy, node.swy2)\n",
    "    denom = max(node.subtree_leaves - 1, 1e-12)\n",
    "    return (sse_leaf - node.subtree_sse) / denom\n",
    "\n",
    "def _collect_internal(node: Node, acc: List[Node]):\n",
    "    if node is None or node.value is not None:\n",
    "        return\n",
    "    acc.append(node)\n",
    "    _collect_internal(node.left, acc)\n",
    "    _collect_internal(node.right, acc)\n",
    "\n",
    "def _prune_once_inplace(root: Node) -> float:\n",
    "    \"\"\"Prune all nodes with minimal α (weakest-link). Returns α*.\"\"\"\n",
    "    _update_subtree_stats(root)\n",
    "    internals: List[Node] = []\n",
    "    _collect_internal(root, internals)\n",
    "    if not internals:\n",
    "        return np.inf\n",
    "    alphas = np.array([_alpha_of(n) for n in internals])\n",
    "    a_star = float(alphas.min())\n",
    "    tol = 1e-12\n",
    "\n",
    "    def prune_mark(n: Node):\n",
    "        if n.value is not None:\n",
    "            return\n",
    "        a = _alpha_of(n)\n",
    "        if abs(a - a_star) <= tol:\n",
    "            # turn node into LEAF using fixed stats\n",
    "            mu = (n.swy / n.sw) if n.sw > 0 else 0.0\n",
    "            n.feature = n.threshold = None\n",
    "            n.left = n.right = None\n",
    "            n.value = float(mu)\n",
    "            n.subtree_sse = leaf_sse_from_stats(n.sw, n.swy, n.swy2)\n",
    "            n.subtree_leaves = 1\n",
    "        else:\n",
    "            prune_mark(n.left); prune_mark(n.right)\n",
    "\n",
    "    prune_mark(root)\n",
    "    _update_subtree_stats(root)\n",
    "    return a_star\n",
    "\n",
    "def pruning_path_stream(root: Node):\n",
    "    \"\"\"\n",
    "    Generator yielding (alpha_star, leaves_now) while pruning in place.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        a = _prune_once_inplace(root)\n",
    "        yield a, root.subtree_leaves\n",
    "        if root.subtree_leaves == 1 or not np.isfinite(a):\n",
    "            break\n",
    "\n",
    "def predict_with_root(X: np.ndarray, root: Node) -> np.ndarray:\n",
    "    X = np.asarray(X, float)\n",
    "    def _one(x, node):\n",
    "        while node.value is None:\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n",
    "    return np.array([_one(X[i], root) for i in range(X.shape[0])], float)\n",
    "\n",
    "def kfold_indices(n_samples: int, n_splits: int, seed=42, shuffle=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n_samples, dtype=int)\n",
    "    if shuffle: rng.shuffle(idx)\n",
    "    folds = np.array_split(idx, n_splits)\n",
    "    for i in range(n_splits):\n",
    "        val = folds[i]\n",
    "        tr  = np.concatenate([folds[j] for j in range(n_splits) if j != i])\n",
    "        yield tr, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruning caps: max_depth=12, min_leaf_weight=16.0 => WMAE=0.181764\n",
      "Pre-pruning caps: max_depth=12, min_leaf_weight=18.0 => WMAE=0.181753\n",
      "Pre-pruning caps: max_depth=12, min_leaf_weight=20.0 => WMAE=0.181798\n",
      "Pre-pruning caps: max_depth=14, min_leaf_weight=16.0 => WMAE=0.181661\n",
      "Pre-pruning caps: max_depth=14, min_leaf_weight=18.0 => WMAE=0.181629\n",
      "Pre-pruning caps: max_depth=14, min_leaf_weight=20.0 => WMAE=0.181757\n",
      "Chosen pre-pruning caps: {'max_depth': 14, 'min_leaf_weight': 18, 'wmae': 0.18162908107788728}\n",
      "\n",
      "Validation metrics (chosen caps):\n",
      " WMAE         : 0.18162908107788728\n",
      " WRMSE        : 0.7404891122545278\n",
      " Weighted R^2 : 0.012237942003804014\n",
      " Poisson Dev. : 43623.02355559506\n",
      "\n",
      "=== Baseline comparisons (VALIDATION) ===\n",
      "Global-mean WMAE (val): 0.189786\n",
      "Group-mean (Area) WMAE (val): 0.189543\n",
      "Model WMAE (val): 0.181629\n",
      "Improvement vs Global (val): 4.30%\n",
      "Improvement vs Group (Area, val): 4.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_6948\\4246597908.py:46: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_6948\\4246597908.py:46: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Load + preprocess + tiny pre-pruning CV (WMAE)\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from preprocessing.preprocessing_utils import preprocess_for_tree  # your function\n",
    "\n",
    "# Load\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test  = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "# Preprocess (no scaling; OHE cats; returns rate & exposure)\n",
    "X_tr, y_tr_rate, w_tr = preprocess_for_tree(train)\n",
    "X_te, y_te_rate, w_te = preprocess_for_tree(test)\n",
    "\n",
    "# Reconstruct counts for Poisson deviance\n",
    "y_tr_cnt = (y_tr_rate * w_tr).to_numpy(float)\n",
    "y_te_cnt = (y_te_rate * w_te).to_numpy(float)\n",
    "\n",
    "# To NumPy (float32 saves memory / faster)\n",
    "X_np = X_tr.values.astype(np.float32)\n",
    "y_np = y_tr_rate.values.astype(np.float32)\n",
    "w_np = w_tr.values.astype(np.float32)\n",
    "\n",
    "# Simple hold-out split for quick validation in the grid\n",
    "rng = np.random.default_rng(42)\n",
    "idx = np.arange(len(y_np))\n",
    "rng.shuffle(idx)\n",
    "cut = int(0.8 * len(idx))\n",
    "tr_idx, va_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "X_tr_np, y_tr_np, w_tr_np = X_np[tr_idx], y_np[tr_idx], w_np[tr_idx]\n",
    "X_va_np, y_va_np, w_va_np = X_np[va_idx], y_np[va_idx], w_np[va_idx]\n",
    "y_va_cnt_np = y_tr_cnt[va_idx]  # counts for Poisson deviance\n",
    "\n",
    "# ---- Tiny grid over pre-pruning caps (selection by WMAE) ----\n",
    "depth_grid = [12, 14]\n",
    "leafw_grid = [16,18,20]\n",
    "\n",
    "best = None\n",
    "for d in depth_grid:\n",
    "    for m in leafw_grid:\n",
    "        tree = DecisionTreeRegressorScratch(max_depth=d, min_leaf_weight=m).fit(X_tr_np, y_tr_np, w_tr_np)\n",
    "        yhat_va = tree.predict(X_va_np)\n",
    "        score = wmae(y_va_np, yhat_va, w_va_np)\n",
    "        if (best is None) or (score < best[\"wmae\"]):\n",
    "            best = {\"max_depth\": d, \"min_leaf_weight\": m, \"wmae\": score, \"yhat_va\": yhat_va}\n",
    "        print(f\"Pre-pruning caps: max_depth={d}, min_leaf_weight={m:.1f} => WMAE={score:.6f}\")    \n",
    "\n",
    "\n",
    "print(\"Chosen pre-pruning caps:\", {k: best[k] for k in [\"max_depth\",\"min_leaf_weight\",\"wmae\"]})\n",
    "\n",
    "# Validation metrics for the chosen caps\n",
    "yhat_va = best[\"yhat_va\"]\n",
    "print(\"\\nValidation metrics (chosen caps):\")\n",
    "print(\" WMAE         :\", wmae(y_va_np, yhat_va, w_va_np))\n",
    "print(\" WRMSE        :\", wrmse(y_va_np, yhat_va, w_va_np))\n",
    "print(\" Weighted R^2 :\", weighted_r2(y_va_np, yhat_va, w_va_np))\n",
    "print(\" Poisson Dev. :\", poisson_deviance(y_va_cnt_np, w_va_np, yhat_va))\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# Baseline comparisons (VALIDATION split)\n",
    "# ===========================================\n",
    "print(\"\\n=== Baseline comparisons (VALIDATION) ===\")\n",
    "\n",
    "# 1) Global-mean baseline (exposure-weighted)\n",
    "ybar_w_val = float((y_va_np * w_va_np).sum() / w_va_np.sum())\n",
    "wmae_global_val = wmae(y_va_np, np.full_like(y_va_np, ybar_w_val), w_va_np)\n",
    "print(f\"Global-mean WMAE (val): {wmae_global_val:.6f}\")\n",
    "\n",
    "# 2) Group-mean baseline by Area (if 'Area' is available in the original train df)\n",
    "#    We align rows by using the validation indices into the original preprocessed X_tr (DataFrame)\n",
    "try:\n",
    "    val_row_labels = X_tr.index[va_idx]\n",
    "    area_val = train.loc[val_row_labels, \"Area\"]\n",
    "\n",
    "    df_val_tmp = pd.DataFrame({\n",
    "        \"Area\": area_val.values,\n",
    "        \"y\":    y_tr_rate.loc[val_row_labels].values,\n",
    "        \"w\":    w_tr.loc[val_row_labels].values\n",
    "    })\n",
    "    # vectorized weighted means per Area\n",
    "    sums = df_val_tmp.assign(yw=df_val_tmp[\"y\"]*df_val_tmp[\"w\"]).groupby(\"Area\")[[\"yw\",\"w\"]].sum()\n",
    "    grp_mean_val = (sums[\"yw\"] / sums[\"w\"]).to_dict()\n",
    "\n",
    "    yhat_group_val = area_val.map(grp_mean_val).to_numpy(dtype=np.float32)\n",
    "    wmae_group_val = wmae(y_va_np, yhat_group_val, w_va_np)\n",
    "    print(f\"Group-mean (Area) WMAE (val): {wmae_group_val:.6f}\")\n",
    "except Exception as e:\n",
    "    print(\"Group-mean baseline (val) skipped (Area not available / alignment issue).\", e)\n",
    "\n",
    "# 3) Compare your model to baselines on validation\n",
    "wmae_model_val = wmae(y_va_np, yhat_va, w_va_np)\n",
    "print(f\"Model WMAE (val): {wmae_model_val:.6f}\")\n",
    "print(f\"Improvement vs Global (val): {1 - (wmae_model_val / wmae_global_val):.2%}\")\n",
    "try:\n",
    "    print(f\"Improvement vs Group (Area, val): {1 - (wmae_model_val / wmae_group_val):.2%}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen α (fast): 0.10828531442923094 | score: 0.1827193429243397\n",
      "\n",
      "Test metrics (final pruned tree - FAST):\n",
      " WMAE         : 0.18312210439549695\n",
      " WRMSE        : 0.7748361550574862\n",
      " Weighted R^2 : 0.012688405450760887\n",
      " Poisson Dev. : 48387.05777551274\n",
      "\n",
      "=== Baseline comparisons (TEST) ===\n",
      "Global-mean WMAE (test): 0.191320\n",
      "Group-mean (Area) WMAE (test): 0.191140\n",
      "Model WMAE (test): 0.183122\n",
      "Improvement vs Global (test): 4.28%\n",
      "Improvement vs Group (Area, test): 4.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_6948\\4246597908.py:46: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_6948\\4246597908.py:46: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# FAST post-pruning via compressed α-grid + small CV\n",
    "# =============================\n",
    "\n",
    "# ---- knobs you can tweak ----\n",
    "K_FOLDS = 5                # 3-fold CV for pruning (faster than 5)\n",
    "MAX_ALPHA_POINTS = 30       # at most this many α’s evaluated\n",
    "USE_HOLDOUT_INSTEAD_OF_CV = True  # set True for ultra-fast single holdout selection\n",
    "\n",
    "def _global_alpha_grid(X, y, w, base_params, max_points=30):\n",
    "    \"\"\"\n",
    "    Build a base tree once and stream its pruning alphas,\n",
    "    then compress to <= max_points via quantiles.\n",
    "    \"\"\"\n",
    "    # stream alphas\n",
    "    alphas = []\n",
    "    root = DecisionTreeRegressorScratch(**base_params).fit(X, y, w).root\n",
    "    for a, _ in pruning_path_stream(root):\n",
    "        if np.isfinite(a): alphas.append(a)\n",
    "        if getattr(root, \"subtree_leaves\", 1) == 1: break\n",
    "    if not alphas:\n",
    "        return np.array([0.0], float)\n",
    "    alphas = np.array(alphas, float)\n",
    "    if alphas.size <= max_points:\n",
    "        return np.unique(alphas)\n",
    "    qs = np.linspace(0, 1, max_points)\n",
    "    return np.unique(np.quantile(alphas, qs))\n",
    "\n",
    "def _score_tree_on(Xv, yv, wv, root):\n",
    "    yhat = predict_with_root(Xv, root)\n",
    "    return wmae(yv, yhat, wv)\n",
    "\n",
    "def select_alpha_fast(X, y, w, base_params, kfold=3, max_points=30, seed=7, use_holdout=False):\n",
    "    \"\"\"\n",
    "    Fast(er) α selection:\n",
    "      - compressed global α grid (<= max_points),\n",
    "      - either HOLDOUT or small k-fold CV with streaming pruning.\n",
    "    \"\"\"\n",
    "    Agrid = _global_alpha_grid(X, y, w, base_params, max_points=max_points)\n",
    "\n",
    "    if use_holdout:\n",
    "        # ---- single holdout ----\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = np.arange(len(y), dtype=int); rng.shuffle(idx)\n",
    "        cut = int(0.8 * len(idx))\n",
    "        tr, va = idx[:cut], idx[cut:]\n",
    "        base = DecisionTreeRegressorScratch(**base_params).fit(X[tr], y[tr], w[tr])\n",
    "        best = {\"alpha\": 0.0, \"score\": _score_tree_on(X[va], y[va], w[va], base.root)}\n",
    "\n",
    "        Agrid_sorted = np.sort(Agrid)\n",
    "        k = 0\n",
    "        if Agrid_sorted[0] == 0.0:\n",
    "            k = 1  # we already scored unpruned\n",
    "        for a, _ in pruning_path_stream(base.root):\n",
    "            if not np.isfinite(a): break\n",
    "            while k < len(Agrid_sorted) and a >= Agrid_sorted[k]:\n",
    "                score = _score_tree_on(X[va], y[va], w[va], base.root)\n",
    "                if score < best[\"score\"]:\n",
    "                    best = {\"alpha\": float(Agrid_sorted[k]), \"score\": float(score)}\n",
    "                k += 1\n",
    "            if getattr(base.root, \"subtree_leaves\", 1) == 1:\n",
    "                break\n",
    "        return {\"alpha\": best[\"alpha\"], \"cv_wmae\": best[\"score\"], \"alphas\": Agrid_sorted}\n",
    "\n",
    "    # ---- small k-fold CV ----\n",
    "    scores = np.zeros(len(Agrid), float)\n",
    "    for tr, va in kfold_indices(len(y), kfold, seed=seed):\n",
    "        base = DecisionTreeRegressorScratch(**base_params).fit(X[tr], y[tr], w[tr])\n",
    "        Agrid_sorted = np.sort(Agrid)\n",
    "        k = 0\n",
    "        last_s = None\n",
    "        if Agrid_sorted[0] == 0.0:\n",
    "            s0 = _score_tree_on(X[va], y[va], w[va], base.root)\n",
    "            scores[0] += s0\n",
    "            last_s = s0\n",
    "            k = 1\n",
    "        for a, _ in pruning_path_stream(base.root):\n",
    "            if not np.isfinite(a): break\n",
    "            # score every Agrid point we just crossed\n",
    "            while k < len(Agrid_sorted) and a >= Agrid_sorted[k]:\n",
    "                s = _score_tree_on(X[va], y[va], w[va], base.root)\n",
    "                scores[k] += s\n",
    "                last_s = s\n",
    "                k += 1\n",
    "            if getattr(base.root, \"subtree_leaves\", 1) == 1:\n",
    "                break\n",
    "        # fill any remaining α points in this fold with the last known score\n",
    "        if last_s is not None and k < len(Agrid_sorted):\n",
    "            scores[k:] += last_s\n",
    "\n",
    "# ===== run the fast selection =====\n",
    "base_params = {\"max_depth\": best[\"max_depth\"], \"min_leaf_weight\": best[\"min_leaf_weight\"]}\n",
    "alpha_sel = select_alpha_fast(\n",
    "    X_np, y_np, w_np,\n",
    "    base_params=base_params,\n",
    "    kfold=K_FOLDS,\n",
    "    max_points=MAX_ALPHA_POINTS,\n",
    "    seed=7,\n",
    "    use_holdout=USE_HOLDOUT_INSTEAD_OF_CV\n",
    ")\n",
    "alpha_star = alpha_sel[\"alpha\"]\n",
    "print(\"Chosen α (fast):\", alpha_star, \"| score:\", alpha_sel[\"cv_wmae\"])\n",
    "\n",
    "# ===== prune a base tree on ALL training up to α* (streaming; in place) =====\n",
    "final_base = DecisionTreeRegressorScratch(**base_params).fit(X_np, y_np, w_np)\n",
    "for a, _ in pruning_path_stream(final_base.root):\n",
    "    if not np.isfinite(a) or a >= alpha_star or getattr(final_base.root, \"subtree_leaves\", 1) == 1:\n",
    "        break\n",
    "\n",
    "# ---- Test evaluation ----\n",
    "X_te_np = X_te.values.astype(np.float32)\n",
    "y_te_np = y_te_rate.values.astype(np.float32)\n",
    "w_te_np = w_te.values.astype(np.float32)\n",
    "yhat_te = predict_with_root(X_te_np, final_base.root)\n",
    "\n",
    "print(\"\\nTest metrics (final pruned tree - FAST):\")\n",
    "print(\" WMAE         :\", wmae(y_te_np, yhat_te, w_te_np))\n",
    "print(\" WRMSE        :\", wrmse(y_te_np, yhat_te, w_te_np))\n",
    "print(\" Weighted R^2 :\", weighted_r2(y_te_np, yhat_te, w_te_np))\n",
    "y_te_cnt_np = (y_te_np * w_te_np).astype(np.float32)\n",
    "print(\" Poisson Dev. :\", poisson_deviance(y_te_cnt_np, w_te_np, yhat_te))\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# Baseline comparisons (TEST split)\n",
    "# ===========================================\n",
    "print(\"\\n=== Baseline comparisons (TEST) ===\")\n",
    "\n",
    "# 1) Global-mean baseline (exposure-weighted)\n",
    "y_te_np = y_te_rate.values.astype(np.float32)\n",
    "w_te_np = w_te.values.astype(np.float32)\n",
    "ybar_w_te = float((y_te_np * w_te_np).sum() / w_te_np.sum())\n",
    "wmae_global_te = wmae(y_te_np, np.full_like(y_te_np, ybar_w_te), w_te_np)\n",
    "print(f\"Global-mean WMAE (test): {wmae_global_te:.6f}\")\n",
    "\n",
    "# 2) Group-mean baseline by Area (if 'Area' is available in the original test df)\n",
    "#    We align rows by using the X_te index into the original 'test' DataFrame\n",
    "try:\n",
    "    area_te = test.loc[X_te.index, \"Area\"]\n",
    "    df_te_tmp = pd.DataFrame({\n",
    "        \"Area\": area_te.values,\n",
    "        \"y\":    y_te_rate.loc[X_te.index].values,\n",
    "        \"w\":    w_te.loc[X_te.index].values\n",
    "    })\n",
    "    sums_te = df_te_tmp.assign(yw=df_te_tmp[\"y\"]*df_te_tmp[\"w\"]).groupby(\"Area\")[[\"yw\",\"w\"]].sum()\n",
    "    grp_mean_te = (sums_te[\"yw\"] / sums_te[\"w\"]).to_dict()\n",
    "\n",
    "    yhat_group_te = area_te.map(grp_mean_te).to_numpy(dtype=np.float32)\n",
    "    wmae_group_te = wmae(y_te_np, yhat_group_te, w_te_np)\n",
    "    print(f\"Group-mean (Area) WMAE (test): {wmae_group_te:.6f}\")\n",
    "except Exception as e:\n",
    "    print(\"Group-mean baseline (test) skipped (Area not available / alignment issue).\", e)\n",
    "\n",
    "# 3) Compare your model to baselines on test\n",
    "wmae_model_te = wmae(y_te_np, yhat_te, w_te_np)\n",
    "print(f\"Model WMAE (test): {wmae_model_te:.6f}\")\n",
    "print(f\"Improvement vs Global (test): {1 - (wmae_model_te / wmae_global_te):.2%}\")\n",
    "try:\n",
    "    print(f\"Improvement vs Group (Area, test): {1 - (wmae_model_te / wmae_group_te):.2%}\")\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL (pre-pruned only, α=0) ===\n",
      "Test WMAE      : 0.1831162719784349\n",
      "Test WRMSE     : 0.7748639259198842\n",
      "Test Weighted R^2: 0.012617631815303398\n",
      "Test Poisson Dev.: 53604.751543446124\n",
      "Leaves (unpruned, chosen caps): 1824\n",
      "Global-mean WMAE (test): 0.191320\n",
      "Group-mean (Area) WMAE (test): 0.191140\n",
      "Improvement vs Global (test): 4.29%\n",
      "Improvement vs Group  (test): 4.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_6948\\4246597908.py:46: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_6948\\4246597908.py:46: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    }
   ],
   "source": [
    "# ===== Final (recommended): pre-pruned only, α = 0 =====\n",
    "base_params = {\"max_depth\": 14, \"min_leaf_weight\": 18.0}\n",
    "\n",
    "final_unpruned = DecisionTreeRegressorScratch(**base_params).fit(X_np, y_np, w_np)\n",
    "yhat_test_unpruned = predict_with_root(X_te.values.astype(np.float32), final_unpruned.root)\n",
    "\n",
    "print(\"\\n=== FINAL (pre-pruned only, α=0) ===\")\n",
    "print(\"Test WMAE      :\", wmae(y_te_rate.values.astype(np.float32), yhat_test_unpruned, w_te.values.astype(np.float32)))\n",
    "print(\"Test WRMSE     :\", wrmse(y_te_rate.values.astype(np.float32), yhat_test_unpruned, w_te.values.astype(np.float32)))\n",
    "print(\"Test Weighted R^2:\", weighted_r2(y_te_rate.values.astype(np.float32), yhat_test_unpruned, w_te.values.astype(np.float32)))\n",
    "y_te_cnt_np = (y_te_rate.values.astype(np.float32) * w_te.values.astype(np.float32))\n",
    "print(\"Test Poisson Dev.:\", poisson_deviance(y_te_cnt_np, w_te.values.astype(np.float32), yhat_test_unpruned))\n",
    "\n",
    "# Leaf counts for interpretability\n",
    "print(\"Leaves (unpruned, chosen caps):\", final_unpruned.root.subtree_leaves)\n",
    "\n",
    "# Baselines on the same test split\n",
    "y_te_np = y_te_rate.values.astype(np.float32)\n",
    "w_te_np = w_te.values.astype(np.float32)\n",
    "ybar_w_te = float((y_te_np * w_te_np).sum() / w_te_np.sum())\n",
    "wmae_global_te = wmae(y_te_np, np.full_like(y_te_np, ybar_w_te), w_te_np)\n",
    "print(\"Global-mean WMAE (test):\", f\"{wmae_global_te:.6f}\")\n",
    "\n",
    "try:\n",
    "    area_te = test.loc[X_te.index, \"Area\"]\n",
    "    df_te_tmp = pd.DataFrame({\"Area\": area_te.values, \"y\": y_te_rate.loc[X_te.index].values, \"w\": w_te.loc[X_te.index].values})\n",
    "    sums_te = df_te_tmp.assign(yw=df_te_tmp[\"y\"]*df_te_tmp[\"w\"]).groupby(\"Area\")[[\"yw\",\"w\"]].sum()\n",
    "    grp_mean_te = (sums_te[\"yw\"] / sums_te[\"w\"]).to_dict()\n",
    "    yhat_group_te = area_te.map(grp_mean_te).to_numpy(dtype=np.float32)\n",
    "    wmae_group_te = wmae(y_te_np, yhat_group_te, w_te_np)\n",
    "    print(\"Group-mean (Area) WMAE (test):\", f\"{wmae_group_te:.6f}\")\n",
    "    print(\"Improvement vs Global (test):\", f\"{1 - (wmae(y_te_np, yhat_test_unpruned, w_te_np) / wmae_global_te):.2%}\")\n",
    "    print(\"Improvement vs Group  (test):\", f\"{1 - (wmae(y_te_np, yhat_test_unpruned, w_te_np) / wmae_group_te):.2%}\")\n",
    "except Exception as e:\n",
    "    print(\"Group-mean baseline (test) skipped:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
