{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97d16344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51eaa7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Minimal, weighted CV pipeline using sklearn trees\n",
    "# - CV over (max_depth, min_leaf_weight in exposure units)\n",
    "# - CV over ccp_alpha from cost_complexity_pruning_path\n",
    "# - Final fit + test metrics\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# ---------- weighted metrics ----------\n",
    "def wmae(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return (np.abs(y - yhat) * w).sum() / np.clip(w.sum(), 1e-12, None)\n",
    "\n",
    "def wrmse(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return np.sqrt(((y - yhat)**2 * w).sum() / np.clip(w.sum(), 1e-12, None))\n",
    "\n",
    "def weighted_r2(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    ybar = (y * w).sum() / np.clip(w.sum(), 1e-12, None)\n",
    "    ss_res = ((y - yhat)**2 * w).sum()\n",
    "    ss_tot = ((y - ybar)**2 * w).sum()\n",
    "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "def poisson_deviance(y_counts, exposure, yhat_rate):\n",
    "    y   = np.asarray(y_counts,  float)\n",
    "    exp = np.asarray(exposure,  float)\n",
    "    lam = np.asarray(yhat_rate, float) * exp\n",
    "    bad = (exp <= 0) & (y > 0)\n",
    "    if np.any(bad):\n",
    "        y, exp, lam = y[~bad], exp[~bad], lam[~bad]\n",
    "    lam = np.clip(lam, 1e-12, None)\n",
    "    term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
    "    return 2.0 * np.nansum(term)\n",
    "\n",
    "def calibration_table(pred_rate, y_counts, exposure, n_bins=12):\n",
    "    pred_rate = np.asarray(pred_rate, float)\n",
    "    y_counts  = np.asarray(y_counts, float)\n",
    "    exposure  = np.asarray(exposure, float)\n",
    "\n",
    "    # ignore impossible rows\n",
    "    ok = exposure > 0\n",
    "    pred_rate, y_counts, exposure = pred_rate[ok], y_counts[ok], exposure[ok]\n",
    "\n",
    "    # bin by predicted rate (exposure-weighted quantiles)\n",
    "    q = np.quantile(pred_rate, np.linspace(0, 1, n_bins + 1))\n",
    "    # make bins unique & robust\n",
    "    q = np.unique(q)\n",
    "    if q.size <= 2:\n",
    "        # not enough variety; single bin fallback\n",
    "        bins = np.zeros_like(pred_rate, int)\n",
    "        edges = np.array([pred_rate.min(), pred_rate.max()])\n",
    "    else:\n",
    "        bins = np.minimum(np.digitize(pred_rate, q[1:-1], right=True), q.size - 2)\n",
    "        edges = q\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"bin\": bins,\n",
    "        \"y\": y_counts,\n",
    "        \"exp\": exposure,\n",
    "        \"pred\": pred_rate\n",
    "    })\n",
    "    agg = df.groupby(\"bin\").agg(\n",
    "        exposure=(\"exp\",\"sum\"),\n",
    "        observed=(\"y\",\"sum\"),\n",
    "        expected=(\"pred\",\"sum\")  # because pred is per-unit exposure; we’ll multiply below\n",
    "    ).reset_index()\n",
    "\n",
    "    # expected counts = sum(pred_rate * exposure)\n",
    "    # we had sum(pred) above, so fix it:\n",
    "    # better: recompute expected per-bin explicitly\n",
    "    agg[\"expected\"] = (\n",
    "        df.groupby(\"bin\").apply(lambda g: np.sum(g[\"pred\"].to_numpy(float) * g[\"exp\"].to_numpy(float)))\n",
    "    ).to_numpy(float)\n",
    "\n",
    "    agg[\"OE\"] = agg[\"observed\"] / np.clip(agg[\"expected\"], 1e-12, None)\n",
    "    return agg[[\"bin\",\"exposure\",\"observed\",\"expected\",\"OE\"]]\n",
    "\n",
    "def pearson_overdispersion(y_counts, exposure, pred_rate):\n",
    "    mu = np.clip(pred_rate * exposure, 1e-12, None)\n",
    "    y  = np.asarray(y_counts, float)\n",
    "    chi2 = np.sum((y - mu)**2 / mu)\n",
    "    df = max(len(y) - 1, 1)\n",
    "    return chi2 / df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre] depth=12, min_leaf_w=16.0 -> CV Dev=37737.585051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\2909726594.py:37: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mean_dev'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m         mean_dev = \u001b[38;5;28mfloat\u001b[39m(np.mean(fold_scores))\n\u001b[32m     62\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[pre] depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, min_leaf_w=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlw\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> CV Dev=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_dev\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mean_dev < \u001b[43mbest_pre\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean_dev\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[32m     64\u001b[39m             best_pre = {\u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(d), \u001b[33m\"\u001b[39m\u001b[33mmin_leaf_weight\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(mlw), \u001b[33m\"\u001b[39m\u001b[33mmean_wmae\u001b[39m\u001b[33m\"\u001b[39m: mean_dev}\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChosen pre-pruning:\u001b[39m\u001b[33m\"\u001b[39m, best_pre)\n",
      "\u001b[31mKeyError\u001b[39m: 'mean_dev'"
     ]
    }
   ],
   "source": [
    "# ---------- data ----------\n",
    "from preprocessing.preprocessing_utils import preprocess_for_tree\n",
    "\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test  = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "X_tr, y_tr_rate, w_tr = preprocess_for_tree(train)\n",
    "X_te, y_te_rate, w_te = preprocess_for_tree(test)\n",
    "\n",
    "# counts for deviance (optional)\n",
    "y_tr_cnt = (y_tr_rate * w_tr).to_numpy(float)\n",
    "y_te_cnt = (y_te_rate * w_te).to_numpy(float)\n",
    "\n",
    "# arrays\n",
    "X = X_tr.values.astype(np.float32)\n",
    "y = y_tr_rate.values.astype(np.float32)\n",
    "w = w_tr.values.astype(np.float32)\n",
    "\n",
    "X_test = X_te.values.astype(np.float32)\n",
    "y_test = y_te_rate.values.astype(np.float32)\n",
    "w_test = w_te.values.astype(np.float32)\n",
    "\n",
    "# ---------- hyperparams ----------\n",
    "DEPTH_GRID = [12, 14]\n",
    "LEAFW_GRID = [16.0, 18.0, 20.0]    # exposure units (sum of weights in a leaf >= this)\n",
    "K_FOLDS    = 5\n",
    "SEED       = 7\n",
    "MAX_ALPHA_POINTS = 30\n",
    "\n",
    "kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# ============================================\n",
    "# 1) CV over (max_depth, min_leaf_weight) with weighted WMAE\n",
    "#    (convert exposure threshold -> min_weight_fraction_leaf per fold)\n",
    "# ============================================\n",
    "best_pre = {\"max_depth\": None, \"min_leaf_weight\": None, \"mean_dev\": np.inf}\n",
    "\n",
    "for d in DEPTH_GRID:\n",
    "    for mlw in LEAFW_GRID:\n",
    "        fold_scores = []\n",
    "        for tr_idx, va_idx in kf.split(X):\n",
    "            Xtr, ytr, wtr = X[tr_idx], y[tr_idx], w[tr_idx]\n",
    "            Xva, yva, wva = X[va_idx], y[va_idx], w[va_idx]\n",
    "\n",
    "            frac = float(mlw / np.clip(wtr.sum(), 1e-12, None))\n",
    "            frac = float(np.clip(frac, 0.0, 0.4999))  # sklearn requires < 0.5\n",
    "\n",
    "            model = DecisionTreeRegressor(\n",
    "                criterion=\"squared_error\",\n",
    "                splitter=\"best\",\n",
    "                max_depth=int(d),\n",
    "                min_weight_fraction_leaf=frac,\n",
    "                ccp_alpha=0.0,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            model.fit(Xtr, ytr, sample_weight=wtr)\n",
    "            yhat = model.predict(Xva)\n",
    "            yva_cnt = yva * wva  # counts on validation fold\n",
    "            fold_scores.append(poisson_deviance(yva_cnt, wva, yhat))\n",
    "\n",
    "        mean_dev = float(np.mean(fold_scores))\n",
    "        print(f\"[pre] depth={d}, min_leaf_w={mlw:.1f} -> CV Dev={mean_dev:.6f}\")\n",
    "        if mean_dev < best_pre[\"mean_dev\"]:\n",
    "            best_pre = {\"max_depth\": int(d), \"min_leaf_weight\": float(mlw), \"mean_dev\": mean_dev}\n",
    "\n",
    "print(\"Chosen pre-pruning:\", best_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2) Build compact α-grid from CCP path (on full train with chosen pre-caps)\n",
    "# ============================================\n",
    "frac_full = float(best_pre[\"min_leaf_weight\"] / np.clip(w.sum(), 1e-12, None))\n",
    "frac_full = float(np.clip(frac_full, 0.0, 0.4999))\n",
    "\n",
    "seed_tree = DecisionTreeRegressor(\n",
    "    criterion=\"squared_error\",\n",
    "    splitter=\"best\",\n",
    "    max_depth=best_pre[\"max_depth\"],\n",
    "    min_weight_fraction_leaf=frac_full,\n",
    "    random_state=SEED\n",
    ")\n",
    "seed_tree.fit(X, y, sample_weight=w)\n",
    "path = seed_tree.cost_complexity_pruning_path(X, y, sample_weight=w)\n",
    "alphas = np.unique(np.asarray(path.ccp_alphas, float))\n",
    "if alphas.size == 0:\n",
    "    alphas = np.array([0.0], float)\n",
    "if alphas[0] > 0.0:\n",
    "    alphas = np.insert(alphas, 0, 0.0)\n",
    "if alphas.size > MAX_ALPHA_POINTS:\n",
    "    qs = np.linspace(0, 1, MAX_ALPHA_POINTS)\n",
    "    alphas = np.unique(np.quantile(alphas, qs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d32c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen α via CV: {'alpha': 3.0369141184607657e-07, 'mean_wmae': 0.1817476217580259}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3) CV over α with same pre-pruning caps (weighted WMAE)\n",
    "# ============================================\n",
    "alpha_scores = np.zeros_like(alphas, dtype=float)\n",
    "\n",
    "for tr_idx, va_idx in kf.split(X):\n",
    "    Xtr, ytr, wtr = X[tr_idx], y[tr_idx], w[tr_idx]\n",
    "    Xva, yva, wva = X[va_idx], y[va_idx], w[va_idx]\n",
    "\n",
    "    frac_fold = float(best_pre[\"min_leaf_weight\"] / np.clip(wtr.sum(), 1e-12, None))\n",
    "    frac_fold = float(np.clip(frac_fold, 0.0, 0.4999))\n",
    "\n",
    "    for i, a in enumerate(alphas):\n",
    "        model = DecisionTreeRegressor(\n",
    "            criterion=\"squared_error\",\n",
    "            splitter=\"best\",\n",
    "            max_depth=best_pre[\"max_depth\"],\n",
    "            min_weight_fraction_leaf=frac_fold,\n",
    "            ccp_alpha=float(a),\n",
    "            random_state=SEED\n",
    "        )\n",
    "        model.fit(Xtr, ytr, sample_weight=wtr)\n",
    "        yhat = model.predict(Xva)\n",
    "        yva_cnt = yva * wva\n",
    "        alpha_scores[i] += poisson_deviance(yva_cnt, wva, yhat)\n",
    "\n",
    "alpha_scores /= K_FOLDS\n",
    "i_best = int(np.argmin(alpha_scores))\n",
    "best_alpha = float(alphas[i_best])\n",
    "print(\"Chosen α via CV:\", {\"alpha\": best_alpha, \"mean_dev\": float(alpha_scores[i_best])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da715f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST METRICS (final pruned) ===\n",
      "WMAE        : 0.1828435645410214\n",
      "WRMSE       : 0.7744725602915187\n",
      "Weighted R^2: 0.013614787368146919\n",
      "Poisson Dev.: 48444.46224273751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\812682618.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_11408\\812682618.py:37: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4) Final fit on ALL training + test evaluation\n",
    "# ============================================\n",
    "final = DecisionTreeRegressor(\n",
    "    criterion=\"squared_error\",\n",
    "    splitter=\"best\",\n",
    "    max_depth=best_pre[\"max_depth\"],\n",
    "    min_weight_fraction_leaf=frac_full,\n",
    "    ccp_alpha=best_alpha,\n",
    "    random_state=SEED\n",
    ")\n",
    "final.fit(X, y, sample_weight=w)\n",
    "\n",
    "yhat_te = final.predict(X_test)\n",
    "calib = calibration_table(yhat_te, y_te_cnt, w_test, n_bins=12)\n",
    "print(\"\\nCalibration by predicted-rate bins (O/E):\")\n",
    "print(calib.to_string(index=False))\n",
    "\n",
    "print(\"Pearson over-dispersion (test):\",\n",
    "      pearson_overdispersion(y_te_cnt, w_test, yhat_te))\n",
    "\n",
    "print(\"\\n=== TEST METRICS (final pruned) ===\")\n",
    "print(\"WMAE        :\", wmae(y_test, yhat_te, w_test))\n",
    "print(\"WRMSE       :\", wrmse(y_test, yhat_te, w_test))\n",
    "print(\"Weighted R^2:\", weighted_r2(y_test, yhat_te, w_test))\n",
    "print(\"Poisson Dev.:\", poisson_deviance(y_te_cnt, w_test, yhat_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591161b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33210b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
