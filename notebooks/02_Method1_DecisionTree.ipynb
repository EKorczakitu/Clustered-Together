{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# From-scratch WEIGHTED Regression Tree\n",
    "# (only stdlib + NumPy inside the method)\n",
    "# =========================\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# -------- primary metric for selection/reporting --------\n",
    "def wmae(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return (np.abs(y - yhat) * w).sum() / w.sum()\n",
    "\n",
    "# -------- small helpers --------\n",
    "def wmean(y, w):\n",
    "    w = np.asarray(w, float)\n",
    "    sw = w.sum()\n",
    "    return (y * w).sum() / sw if sw > 0 else 0.0\n",
    "\n",
    "def leaf_sse(y, w):\n",
    "    mu = wmean(y, w)\n",
    "    return ((y - mu) ** 2 * w).sum()\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    feature: Optional[int] = None\n",
    "    threshold: Optional[float] = None\n",
    "    left: Optional[\"Node\"] = None\n",
    "    right: Optional[\"Node\"] = None\n",
    "    value: Optional[float] = None  # prediction at leaf\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeRegressorScratch:\n",
    "    \"\"\"\n",
    "    Simple exposure-weighted regression tree for rates.\n",
    "    - Splits minimize weighted SSE (sum of leaf SSEs).\n",
    "    - Leaf prediction = exposure-weighted mean of y in the region.\n",
    "    - Pre-pruning via max_depth and min_leaf_weight (exposure units).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth: Optional[int] = None, min_leaf_weight: float = 5.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_weight = float(min_leaf_weight)\n",
    "        self.root: Optional[Node] = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, w: np.ndarray):\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y, float)\n",
    "        w = np.asarray(w, float)\n",
    "        self.root = self._build_tree(X, y, w, np.arange(X.shape[0]), depth=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, float)\n",
    "        return np.array([self._traverse(x, self.root) for x in X], float)\n",
    "\n",
    "    # ----- internal: build tree -----\n",
    "    def _build_tree(self, X, y, w, idx, depth) -> Node:\n",
    "        # stopping rules\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (w[idx].sum() < 2 * self.min_leaf_weight) or \\\n",
    "           np.allclose(y[idx], y[idx][0]):\n",
    "            return Node(value=wmean(y[idx], w[idx]))\n",
    "\n",
    "        feat, thr, L, R = self._best_split(X, y, w, idx)\n",
    "        if feat is None:\n",
    "            return Node(value=wmean(y[idx], w[idx]))\n",
    "\n",
    "        left = self._build_tree(X, y, w, L, depth+1)\n",
    "        right = self._build_tree(X, y, w, R, depth+1)\n",
    "        return Node(feature=feat, threshold=thr, left=left, right=right)\n",
    "\n",
    "    # ----- internal: best split -----\n",
    "    def _best_split(self, X, y, w, idx) -> Tuple[Optional[int], Optional[float], Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        best = (None, None, None, None, np.inf)\n",
    "        n, d = X.shape\n",
    "        for j in range(d):\n",
    "            xj = X[idx, j]\n",
    "            uniq = np.unique(xj)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "            # thresholds: midpoints; for one-hot 0/1, just 0.5\n",
    "            if uniq.size == 2 and uniq.min() == 0.0 and uniq.max() == 1.0:\n",
    "                candidates = [0.5]\n",
    "            else:\n",
    "                u = np.unique(np.sort(xj))\n",
    "                candidates = (u[:-1] + u[1:]) / 2.0\n",
    "\n",
    "            for t in candidates:\n",
    "                Lmask = xj <= t\n",
    "                if not Lmask.any() or Lmask.all():\n",
    "                    continue\n",
    "                L = idx[Lmask]; R = idx[~Lmask]\n",
    "                # exposure-weighted minimum leaf size\n",
    "                if w[L].sum() < self.min_leaf_weight or w[R].sum() < self.min_leaf_weight:\n",
    "                    continue\n",
    "                sse = leaf_sse(y[L], w[L]) + leaf_sse(y[R], w[R])\n",
    "                if sse < best[4]:\n",
    "                    best = (j, t, L, R, sse)\n",
    "\n",
    "        return best[0], best[1], best[2], best[3]\n",
    "\n",
    "    # ----- internal: predict one -----\n",
    "    def _traverse(self, x, node: Node) -> float:\n",
    "        while not node.is_leaf():\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_for_tree\n\u001b[32m      4\u001b[39m train = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/claims_train.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m test  = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/claims_test.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from preprocessing.preprocessing_utils import preprocess_for_tree\n",
    "\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test  = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "X_tr, y_tr, w_tr = preprocess_for_tree(train)\n",
    "X_te, y_te, w_te = preprocess_for_tree(test)\n",
    "\n",
    "# Convert to NumPy\n",
    "X_np = X_tr.values.astype(float)\n",
    "y_np = y_tr.values.astype(float)\n",
    "w_np = w_tr.values.astype(float)\n",
    "\n",
    "# Simple train/val split (you may use sklearn's train_test_split if you want)\n",
    "rng = np.random.default_rng(42)\n",
    "idx = np.arange(len(y_np))\n",
    "rng.shuffle(idx)\n",
    "cut = int(0.8 * len(idx))\n",
    "tr_idx, va_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "X_tr_np, y_tr_np, w_tr_np = X_np[tr_idx], y_np[tr_idx], w_np[tr_idx]\n",
    "X_va_np, y_va_np, w_va_np = X_np[va_idx], y_np[va_idx], w_np[va_idx]\n",
    "\n",
    "# ---- tiny CV grid (keep it small & readable) ----\n",
    "depth_grid = [3, 5, 7, 9]\n",
    "leafw_grid = [5.0, 10.0, 20.0]\n",
    "\n",
    "best = None\n",
    "for d in depth_grid:\n",
    "    for m in leafw_grid:\n",
    "        tree = DecisionTreeRegressorScratch(max_depth=d, min_leaf_weight=m).fit(X_tr_np, y_tr_np, w_tr_np)\n",
    "        yhat = tree.predict(X_va_np)\n",
    "        score = wmae(y_va_np, yhat, w_va_np)\n",
    "        if (best is None) or (score < best[\"wmae\"]):\n",
    "            best = {\"max_depth\": d, \"min_leaf_weight\": m, \"wmae\": score, \"model\": tree}\n",
    "\n",
    "print(\"Chosen caps:\", {k: best[k] for k in [\"max_depth\",\"min_leaf_weight\",\"wmae\"]})\n",
    "\n",
    "# ---- final fit on FULL training set with chosen caps ----\n",
    "final_tree = DecisionTreeRegressorScratch(\n",
    "    max_depth=best[\"max_depth\"],\n",
    "    min_leaf_weight=best[\"min_leaf_weight\"]\n",
    ").fit(X_np, y_np, w_np)\n",
    "\n",
    "# ---- evaluate on your held-out test (if you have labels) ----\n",
    "if \"y_te\" in globals() and y_te is not None:\n",
    "    X_te_np = X_te.values.astype(float)\n",
    "    y_te_np = y_te.values.astype(float)\n",
    "    w_te_np = w_te.values.astype(float)\n",
    "    yhat_te = final_tree.predict(X_te_np)\n",
    "    print(\"Test WMAE:\", wmae(y_te_np, yhat_te, w_te_np))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
