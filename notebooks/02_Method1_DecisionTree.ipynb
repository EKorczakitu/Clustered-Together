{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# From-scratch WEIGHTED Regression Tree (NumPy only)\n",
    "# + metrics\n",
    "# + cost-complexity pruning (α) with CV\n",
    "# =========================\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# -------- Metrics --------\n",
    "def wmae(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return (np.abs(y - yhat) * w).sum() / w.sum()\n",
    "\n",
    "def wrmse(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return np.sqrt(((y - yhat)**2 * w).sum() / w.sum())\n",
    "\n",
    "def weighted_r2(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    ybar = (y * w).sum() / w.sum()\n",
    "    ss_res = ((y - yhat)**2 * w).sum()\n",
    "    ss_tot = ((y - ybar)**2 * w).sum()\n",
    "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "def poisson_deviance(y_counts, exposure, yhat_rate):\n",
    "    y = np.asarray(y_counts, float)\n",
    "    exp = np.asarray(exposure, float)\n",
    "    lam = np.clip(yhat_rate, 1e-12, None) * exp  # Poisson mean = rate * exposure\n",
    "    term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
    "    return 2.0 * term.sum()\n",
    "\n",
    "# -------- Helpers --------\n",
    "def wmean(y, w):\n",
    "    w = np.asarray(w, float)\n",
    "    sw = w.sum()\n",
    "    return (y * w).sum() / sw if sw > 0 else 0.0\n",
    "\n",
    "def leaf_sse(y, w):\n",
    "    mu = wmean(y, w)\n",
    "    return ((y - mu) ** 2 * w).sum()\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    feature: Optional[int] = None\n",
    "    threshold: Optional[float] = None\n",
    "    left: Optional[\"Node\"] = None\n",
    "    right: Optional[\"Node\"] = None\n",
    "    value: Optional[float] = None  # prediction at leaf\n",
    "    # for pruning bookkeeping\n",
    "    idx: Optional[np.ndarray] = None\n",
    "    sse: float = 0.0\n",
    "    leaves: int = 1\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeRegressorScratch:\n",
    "    \"\"\"\n",
    "    Exposure-weighted regression tree for claim rate.\n",
    "      - Split criterion: minimize weighted SSE.\n",
    "      - Leaf value: exposure-weighted mean.\n",
    "      - Pre-pruning: max_depth, min_leaf_weight (exposure units).\n",
    "    Uses only NumPy for training/prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth: Optional[int] = None, min_leaf_weight: float = 5.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_weight = float(min_leaf_weight)\n",
    "        self.root: Optional[Node] = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, w: np.ndarray):\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y, float)\n",
    "        w = np.asarray(w, float)\n",
    "        self.root = self._build_tree(X, y, w, np.arange(X.shape[0], dtype=int), depth=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, float)\n",
    "        return np.array([self._traverse(x, self.root) for x in X], float)\n",
    "\n",
    "    # ----- internal: build tree -----\n",
    "    def _build_tree(self, X, y, w, idx, depth) -> Node:\n",
    "        # stopping rules\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (w[idx].sum() < 2 * self.min_leaf_weight) or \\\n",
    "           np.allclose(y[idx], y[idx][0]):\n",
    "            return Node(value=wmean(y[idx], w[idx]), idx=idx.copy())\n",
    "\n",
    "        feat, thr, L, R = self._best_split(X, y, w, idx)\n",
    "        if feat is None:\n",
    "            return Node(value=wmean(y[idx], w[idx]), idx=idx.copy())\n",
    "\n",
    "        left = self._build_tree(X, y, w, L, depth+1)\n",
    "        right = self._build_tree(X, y, w, R, depth+1)\n",
    "        return Node(feature=feat, threshold=thr, left=left, right=right, value=None, idx=idx.copy())\n",
    "\n",
    "    # ----- internal: best split -----\n",
    "    def _best_split(self, X, y, w, idx) -> Tuple[Optional[int], Optional[float], Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        best = (None, None, None, None, np.inf)\n",
    "        n, d = X.shape\n",
    "        for j in range(d):\n",
    "            xj = X[idx, j]\n",
    "            uniq = np.unique(xj)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "            # thresholds: midpoints; for one-hot 0/1, just 0.5\n",
    "            if uniq.size == 2 and uniq.min() == 0.0 and uniq.max() == 1.0:\n",
    "                candidates = [0.5]\n",
    "            else:\n",
    "                u = np.unique(np.sort(xj))\n",
    "                candidates = (u[:-1] + u[1:]) / 2.0\n",
    "\n",
    "            for t in candidates:\n",
    "                Lmask = xj <= t\n",
    "                if not Lmask.any() or Lmask.all():\n",
    "                    continue\n",
    "                L = idx[Lmask]; R = idx[~Lmask]\n",
    "                # min exposure per child\n",
    "                if w[L].sum() < self.min_leaf_weight or w[R].sum() < self.min_leaf_weight:\n",
    "                    continue\n",
    "                sse = leaf_sse(y[L], w[L]) + leaf_sse(y[R], w[R])\n",
    "                if sse < best[4]:\n",
    "                    best = (j, t, L, R, sse)\n",
    "\n",
    "        return best[0], best[1], best[2], best[3]\n",
    "\n",
    "    # ----- internal: predict one -----\n",
    "    def _traverse(self, x, node: Node) -> float:\n",
    "        while not node.is_leaf():\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n",
    "\n",
    "# -------- Pruning utilities (weakest-link CCP) --------\n",
    "def _compute_stats(node: Node, y, w):\n",
    "    \"\"\"Bottom-up subtree SSE and leaf counts (uses stored node.idx).\"\"\"\n",
    "    if node.value is not None:\n",
    "        node.sse = leaf_sse(y[node.idx], w[node.idx])\n",
    "        node.leaves = 1\n",
    "        return node.sse, 1\n",
    "    sL, lL = _compute_stats(node.left,  y, w)\n",
    "    sR, lR = _compute_stats(node.right, y, w)\n",
    "    node.sse = sL + sR\n",
    "    node.leaves = lL + lR\n",
    "    return node.sse, node.leaves\n",
    "\n",
    "def _alpha_of(node: Node, y, w) -> float:\n",
    "    \"\"\"α_t = (SSE_if_pruned_to_leaf - SSE_subtree) / (leaves_subtree - 1).\"\"\"\n",
    "    if node.value is not None:\n",
    "        return np.inf\n",
    "    # SSE if this node becomes a leaf:\n",
    "    mu = wmean(y[node.idx], w[node.idx])\n",
    "    sse_leaf = ((y[node.idx] - mu)**2 * w[node.idx]).sum()\n",
    "    denom = max(node.leaves - 1, 1e-12)\n",
    "    return (sse_leaf - node.sse) / denom\n",
    "\n",
    "def _collect_internal(node: Node) -> List[Node]:\n",
    "    if node is None or node.value is not None:\n",
    "        return []\n",
    "    return [node] + _collect_internal(node.left) + _collect_internal(node.right)\n",
    "\n",
    "def _clone(node: Node) -> Node:\n",
    "    if node is None:\n",
    "        return None\n",
    "    out = Node(node.feature, node.threshold,\n",
    "               _clone(node.left), _clone(node.right),\n",
    "               node.value,\n",
    "               idx=(None if node.idx is None else node.idx.copy()),\n",
    "               sse=node.sse, leaves=node.leaves)\n",
    "    return out\n",
    "\n",
    "def _prune_once(root: Node, y, w) -> Tuple[Node, float]:\n",
    "    \"\"\"Prune all internal nodes with minimal α; return (new_root, α*).\"\"\"\n",
    "    _compute_stats(root, y, w)\n",
    "    internals = _collect_internal(root)\n",
    "    if not internals:\n",
    "        return root, np.inf\n",
    "    alphas = np.array([_alpha_of(n, y, w) for n in internals])\n",
    "    alpha_star = float(np.min(alphas))\n",
    "    tol = 1e-12\n",
    "\n",
    "    def prune_mark(n: Node):\n",
    "        if n.value is not None:\n",
    "            return\n",
    "        a = _alpha_of(n, y, w)\n",
    "        if abs(a - alpha_star) <= tol:\n",
    "            # turn into a leaf\n",
    "            mu = wmean(y[n.idx], w[n.idx])\n",
    "            n.feature = n.threshold = None\n",
    "            n.left = n.right = None\n",
    "            n.value = float(mu)\n",
    "            n.sse = leaf_sse(y[n.idx], w[n.idx])\n",
    "            n.leaves = 1\n",
    "        else:\n",
    "            prune_mark(n.left); prune_mark(n.right)\n",
    "\n",
    "    prune_mark(root)\n",
    "    _compute_stats(root, y, w)\n",
    "    return root, alpha_star\n",
    "\n",
    "def pruning_path(root: Node, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> Tuple[List[Node], List[float]]:\n",
    "    \"\"\"\n",
    "    Build CCP path: sequences of trees and α values starting from unpruned root.\n",
    "    \"\"\"\n",
    "    cur = _clone(root)\n",
    "    # (node.idx already stored during fit)\n",
    "    _compute_stats(cur, y, w)\n",
    "    trees = [_clone(cur)]\n",
    "    alphas = [0.0]\n",
    "    while True:\n",
    "        cur, a = _prune_once(cur, y, w)\n",
    "        trees.append(_clone(cur))\n",
    "        alphas.append(a)\n",
    "        if getattr(cur, \"leaves\", 1) == 1:\n",
    "            break\n",
    "    return trees, alphas\n",
    "\n",
    "def predict_with_root(X: np.ndarray, root: Node) -> np.ndarray:\n",
    "    X = np.asarray(X, float)\n",
    "    def _one(x, node):\n",
    "        while node.value is None:\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n",
    "    return np.array([_one(X[i], root) for i in range(X.shape[0])], float)\n",
    "\n",
    "def kfold_indices(n_samples: int, n_splits: int, seed=42, shuffle=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n_samples, dtype=int)\n",
    "    if shuffle: rng.shuffle(idx)\n",
    "    folds = np.array_split(idx, n_splits)\n",
    "    for i in range(n_splits):\n",
    "        val = folds[i]\n",
    "        tr  = np.concatenate([folds[j] for j in range(n_splits) if j != i])\n",
    "        yield tr, val\n",
    "\n",
    "def cv_select_alpha(X, y, w, base_params, kfold=5, seed=7):\n",
    "    \"\"\"\n",
    "    Select α by CV (WMAE):\n",
    "      1) Build global α list from a base tree on ALL data.\n",
    "      2) For each fold, grow base tree on train fold, build its path,\n",
    "         and for each global α choose the smallest subtree with α' >= α.\n",
    "    \"\"\"\n",
    "    # global α candidates\n",
    "    base_all = DecisionTreeRegressorScratch(**base_params).fit(X, y, w)\n",
    "    path_all, alphas_all = pruning_path(base_all.root, X, y, w)\n",
    "\n",
    "    scores = np.zeros(len(alphas_all))\n",
    "    for tr, va in kfold_indices(len(y), kfold, seed=seed):\n",
    "        base = DecisionTreeRegressorScratch(**base_params).fit(X[tr], y[tr], w[tr])\n",
    "        t_path, a_path = pruning_path(base.root, X[tr], y[tr], w[tr])\n",
    "\n",
    "        # for each global α, pick first subtree with α' >= α in this fold\n",
    "        for k, a in enumerate(alphas_all):\n",
    "            idxs = [j for j, ap in enumerate(a_path) if ap >= a]\n",
    "            jstar = idxs[0] if idxs else (len(a_path)-1)\n",
    "            yhat = predict_with_root(X[va], t_path[jstar])\n",
    "            scores[k] += wmae(y[va], yhat, w[va])\n",
    "\n",
    "    scores /= kfold\n",
    "    kbest = int(np.argmin(scores))\n",
    "    return {\"alpha\": float(alphas_all[kbest]),\n",
    "            \"cv_wmae\": float(scores[kbest]),\n",
    "            \"alphas\": [float(a) for a in alphas_all],\n",
    "            \"cv_curve\": scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen pre-pruning caps: {'max_depth': 19, 'min_leaf_weight': 20.0, 'wmae': np.float64(0.18151267421363473)}\n",
      "\n",
      "Validation metrics (chosen caps):\n",
      " WMAE         : 0.18151267421363473\n",
      " WRMSE        : 0.7420427339046141\n",
      " Weighted R^2 : 0.008088772458327065\n",
      " Poisson Dev. : 62097.86846094347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5516\\1500063327.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_5516\\1500063327.py:30: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Load + preprocess + tiny pre-pruning CV\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from preprocessing.preprocessing_utils import preprocess_for_tree  # your function\n",
    "\n",
    "# Load\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test  = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "# Preprocess (no scaling; OHE cats; returns rate & exposure)\n",
    "X_tr, y_tr_rate, w_tr = preprocess_for_tree(train)\n",
    "X_te, y_te_rate, w_te = preprocess_for_tree(test)\n",
    "\n",
    "# Reconstruct counts for Poisson deviance\n",
    "y_tr_cnt = (y_tr_rate * w_tr).to_numpy(float)\n",
    "y_te_cnt = (y_te_rate * w_te).to_numpy(float)\n",
    "\n",
    "# To NumPy\n",
    "X_np = X_tr.values.astype(float)\n",
    "y_np = y_tr_rate.values.astype(float)\n",
    "w_np = w_tr.values.astype(float)\n",
    "\n",
    "# Simple hold-out split for quick validation in the grid\n",
    "rng = np.random.default_rng(42)\n",
    "idx = np.arange(len(y_np))\n",
    "rng.shuffle(idx)\n",
    "cut = int(0.8 * len(idx))\n",
    "tr_idx, va_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "X_tr_np, y_tr_np, w_tr_np = X_np[tr_idx], y_np[tr_idx], w_np[tr_idx]\n",
    "X_va_np, y_va_np, w_va_np = X_np[va_idx], y_np[va_idx], w_np[va_idx]\n",
    "y_va_cnt_np = y_tr_cnt[va_idx]  # counts for Poisson deviance\n",
    "\n",
    "# ---- Tiny grid over pre-pruning caps (selection by WMAE) ----\n",
    "depth_grid = [9, 11, 13, 15, 17, 19, 21]\n",
    "leafw_grid = [7.0, 9.0, 11.0, 13.0, 15.0, 20.0, 50.0]\n",
    "\n",
    "best = None\n",
    "for d in depth_grid:\n",
    "    for m in leafw_grid:\n",
    "        tree = DecisionTreeRegressorScratch(max_depth=d, min_leaf_weight=m).fit(X_tr_np, y_tr_np, w_tr_np)\n",
    "        yhat_va = tree.predict(X_va_np)\n",
    "        score = wmae(y_va_np, yhat_va, w_va_np)\n",
    "        if (best is None) or (score < best[\"wmae\"]):\n",
    "            best = {\"max_depth\": d, \"min_leaf_weight\": m, \"wmae\": score, \"yhat_va\": yhat_va}\n",
    "\n",
    "print(\"Chosen pre-pruning caps:\", {k: best[k] for k in [\"max_depth\",\"min_leaf_weight\",\"wmae\"]})\n",
    "\n",
    "# Report validation metrics for the chosen caps\n",
    "yhat_va = best[\"yhat_va\"]\n",
    "print(\"\\nValidation metrics (chosen caps):\")\n",
    "print(\" WMAE         :\", wmae(y_va_np, yhat_va, w_va_np))\n",
    "print(\" WRMSE        :\", wrmse(y_va_np, yhat_va, w_va_np))\n",
    "print(\" Weighted R^2 :\", weighted_r2(y_va_np, yhat_va, w_va_np))\n",
    "print(\" Poisson Dev. :\", poisson_deviance(y_va_cnt_np, w_va_np, yhat_va))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.43 MiB for an array with shape (187280,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m base_full = DecisionTreeRegressorScratch(**base_params).fit(X_np, y_np, w_np)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Build pruning path on full train to get α candidates (and trees)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m path_trees, path_alphas = pruning_path(base_full.root, X_np, y_np, w_np)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Cross-validate to select α (selection by WMAE)\u001b[39;00m\n\u001b[32m     12\u001b[39m alpha_sel = cv_select_alpha(X_np, y_np, w_np, base_params=base_params, kfold=\u001b[32m5\u001b[39m, seed=\u001b[32m7\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 212\u001b[39m, in \u001b[36mpruning_path\u001b[39m\u001b[34m(root, X, y, w)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    211\u001b[39m     cur, a = _prune_once(cur, y, w)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     trees.append(_clone(cur))\n\u001b[32m    213\u001b[39m     alphas.append(a)\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(cur, \u001b[33m\"\u001b[39m\u001b[33mleaves\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36m_clone\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    165\u001b[39m out = Node(node.feature, node.threshold,\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m            _clone(node.left), _clone(node.right),\n\u001b[32m    167\u001b[39m            node.value,\n\u001b[32m    168\u001b[39m            idx=(\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m node.idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m node.idx.copy()),\n\u001b[32m    169\u001b[39m            sse=node.sse, leaves=node.leaves)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36m_clone\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    165\u001b[39m out = Node(node.feature, node.threshold,\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m            _clone(node.left), _clone(node.right),\n\u001b[32m    167\u001b[39m            node.value,\n\u001b[32m    168\u001b[39m            idx=(\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m node.idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m node.idx.copy()),\n\u001b[32m    169\u001b[39m            sse=node.sse, leaves=node.leaves)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping similar frames: _clone at line 166 (4 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36m_clone\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    165\u001b[39m out = Node(node.feature, node.threshold,\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m            _clone(node.left), _clone(node.right),\n\u001b[32m    167\u001b[39m            node.value,\n\u001b[32m    168\u001b[39m            idx=(\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m node.idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m node.idx.copy()),\n\u001b[32m    169\u001b[39m            sse=node.sse, leaves=node.leaves)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 168\u001b[39m, in \u001b[36m_clone\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    165\u001b[39m out = Node(node.feature, node.threshold,\n\u001b[32m    166\u001b[39m            _clone(node.left), _clone(node.right),\n\u001b[32m    167\u001b[39m            node.value,\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m            idx=(\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m node.idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m node.idx.copy()),\n\u001b[32m    169\u001b[39m            sse=node.sse, leaves=node.leaves)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.43 MiB for an array with shape (187280,) and data type int64"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Post-pruning via cost-complexity (α) + final eval\n",
    "# =============================\n",
    "# Fit base tree on ALL training with chosen caps\n",
    "base_params = {\"max_depth\": best[\"max_depth\"], \"min_leaf_weight\": best[\"min_leaf_weight\"]}\n",
    "base_full = DecisionTreeRegressorScratch(**base_params).fit(X_np, y_np, w_np)\n",
    "\n",
    "# Build pruning path on full train to get α candidates (and trees)\n",
    "path_trees, path_alphas = pruning_path(base_full.root, X_np, y_np, w_np)\n",
    "\n",
    "# Cross-validate to select α (selection by WMAE)\n",
    "alpha_sel = cv_select_alpha(X_np, y_np, w_np, base_params=base_params, kfold=5, seed=7)\n",
    "alpha_star = alpha_sel[\"alpha\"]\n",
    "print(\"Chosen α:\", alpha_star, \"| CV WMAE:\", alpha_sel[\"cv_wmae\"])\n",
    "\n",
    "# Pick the smallest subtree with α' >= α*\n",
    "idx_candidates = [i for i, a in enumerate(path_alphas) if a >= alpha_star]\n",
    "kstar = idx_candidates[0] if idx_candidates else (len(path_alphas) - 1)\n",
    "final_root = path_trees[kstar]\n",
    "\n",
    "# ---- Test evaluation ----\n",
    "X_te_np = X_te.values.astype(float)\n",
    "y_te_np = y_te_rate.values.astype(float)\n",
    "w_te_np = w_te.values.astype(float)\n",
    "yhat_te = predict_with_root(X_te_np, final_root)\n",
    "\n",
    "print(\"\\nTest metrics (final pruned tree):\")\n",
    "print(\" WMAE         :\", wmae(y_te_np, yhat_te, w_te_np))\n",
    "print(\" WRMSE        :\", wrmse(y_te_np, yhat_te, w_te_np))\n",
    "print(\" Weighted R^2 :\", weighted_r2(y_te_np, yhat_te, w_te_np))\n",
    "y_te_cnt_np = (y_te_np * w_te_np).astype(float)\n",
    "print(\" Poisson Dev. :\", poisson_deviance(y_te_cnt_np, w_te_np, yhat_te))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
