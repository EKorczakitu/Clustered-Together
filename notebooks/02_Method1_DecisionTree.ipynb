{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# From-scratch WEIGHTED Regression Tree (NumPy only)\n",
    "# + metrics\n",
    "# + memory-light cost-complexity pruning (α) with CV\n",
    "# =========================\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "# -------- Metrics --------\n",
    "def wmae(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return (np.abs(y - yhat) * w).sum() / w.sum()\n",
    "\n",
    "def wrmse(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return np.sqrt(((y - yhat)**2 * w).sum() / w.sum())\n",
    "\n",
    "def weighted_r2(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    ybar = (y * w).sum() / w.sum()\n",
    "    ss_res = ((y - yhat)**2 * w).sum()\n",
    "    ss_tot = ((y - ybar)**2 * w).sum()\n",
    "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "def poisson_deviance(y_counts, exposure, yhat_rate):\n",
    "    y = np.asarray(y_counts, float)\n",
    "    exp = np.asarray(exposure, float)\n",
    "    lam = np.clip(yhat_rate, 1e-12, None) * exp\n",
    "    term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
    "    return 2.0 * term.sum()\n",
    "\n",
    "# -------- Helpers --------\n",
    "def wmean(y, w):\n",
    "    sw = w.sum()\n",
    "    return (y * w).sum() / sw if sw > 0 else 0.0\n",
    "\n",
    "def leaf_sse_from_stats(sw, swy, swy2):\n",
    "    # SSE = sum w(y - mean)^2 = sum(wy^2) - (sum(wy))^2 / sum(w)\n",
    "    return float(swy2 - (swy * swy) / sw) if sw > 0 else 0.0\n",
    "\n",
    "@dataclass\n",
    "class Node:\n",
    "    feature: Optional[int] = None\n",
    "    threshold: Optional[float] = None\n",
    "    left: Optional[\"Node\"] = None\n",
    "    right: Optional[\"Node\"] = None\n",
    "    value: Optional[float] = None   # leaf prediction (weighted mean)\n",
    "\n",
    "    # sufficient stats for THIS NODE'S SAMPLE SET (fixed after fit)\n",
    "    sw: float = 0.0                  # sum of weights\n",
    "    swy: float = 0.0                 # sum of w*y\n",
    "    swy2: float = 0.0                # sum of w*y^2\n",
    "\n",
    "    # subtree stats (change as we prune)\n",
    "    subtree_sse: float = 0.0\n",
    "    subtree_leaves: int = 1\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.value is not None\n",
    "\n",
    "class DecisionTreeRegressorScratch:\n",
    "    \"\"\"\n",
    "    Exposure-weighted regression tree for claim rate.\n",
    "    - Split criterion: minimize weighted SSE.\n",
    "    - Leaf value: exposure-weighted mean.\n",
    "    - Pre-pruning: max_depth, min_leaf_weight (exposure units).\n",
    "    Uses only NumPy for training/prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth: Optional[int] = None, min_leaf_weight: float = 5.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_weight = float(min_leaf_weight)\n",
    "        self.root: Optional[Node] = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, w: np.ndarray):\n",
    "        X = np.asarray(X, float)\n",
    "        y = np.asarray(y, float)\n",
    "        w = np.asarray(w, float)\n",
    "        idx = np.arange(X.shape[0], dtype=int)\n",
    "        self.root = self._build_tree(X, y, w, idx, depth=0)\n",
    "        # initialize subtree stats once\n",
    "        _update_subtree_stats(self.root)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, float)\n",
    "        return np.array([self._traverse(x, self.root) for x in X], float)\n",
    "\n",
    "    # ----- internal: build tree -----\n",
    "    def _build_tree(self, X, y, w, idx, depth) -> Node:\n",
    "        sw = w[idx].sum()\n",
    "        swy = (y[idx] * w[idx]).sum()\n",
    "        swy2 = ((y[idx] ** 2) * w[idx]).sum()\n",
    "\n",
    "        # stopping rules\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           (sw < 2 * self.min_leaf_weight) or \\\n",
    "           np.allclose(y[idx], y[idx][0]):\n",
    "            return Node(value=(swy / sw if sw > 0 else 0.0), sw=sw, swy=swy, swy2=swy2,\n",
    "                        subtree_sse=leaf_sse_from_stats(sw, swy, swy2), subtree_leaves=1)\n",
    "\n",
    "        feat, thr, L, R = self._best_split(X, y, w, idx)\n",
    "        if feat is None:\n",
    "            return Node(value=(swy / sw if sw > 0 else 0.0), sw=sw, swy=swy, swy2=swy2,\n",
    "                        subtree_sse=leaf_sse_from_stats(sw, swy, swy2), subtree_leaves=1)\n",
    "\n",
    "        left  = self._build_tree(X, y, w, L, depth+1)\n",
    "        right = self._build_tree(X, y, w, R, depth+1)\n",
    "        return Node(feature=feat, threshold=thr, left=left, right=right,\n",
    "                    value=None, sw=sw, swy=swy, swy2=swy2)\n",
    "\n",
    "    # ----- internal: best split -----\n",
    "    def _best_split(self, X, y, w, idx) -> Tuple[Optional[int], Optional[float], Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        best = (None, None, None, None, np.inf)\n",
    "        n, d = X.shape\n",
    "        for j in range(d):\n",
    "            xj = X[idx, j]\n",
    "            uniq = np.unique(xj)\n",
    "            if uniq.size <= 1:\n",
    "                continue\n",
    "            # thresholds: midpoints; for one-hot 0/1, just 0.5\n",
    "            if uniq.size == 2 and uniq.min() == 0.0 and uniq.max() == 1.0:\n",
    "                candidates = [0.5]\n",
    "            else:\n",
    "                u = np.unique(np.sort(xj))\n",
    "                candidates = (u[:-1] + u[1:]) / 2.0\n",
    "\n",
    "            for t in candidates:\n",
    "                Lmask = xj <= t\n",
    "                if not Lmask.any() or Lmask.all():\n",
    "                    continue\n",
    "                L = idx[Lmask]; R = idx[~Lmask]\n",
    "\n",
    "                swL = w[L].sum(); swR = w[R].sum()\n",
    "                if swL < self.min_leaf_weight or swR < self.min_leaf_weight:\n",
    "                    continue\n",
    "\n",
    "                swyL = (y[L] * w[L]).sum(); swyR = (y[R] * w[R]).sum()\n",
    "                swy2L = ((y[L]**2) * w[L]).sum(); swy2R = ((y[R]**2) * w[R]).sum()\n",
    "                sseL = leaf_sse_from_stats(swL, swyL, swy2L)\n",
    "                sseR = leaf_sse_from_stats(swR, swyR, swy2R)\n",
    "                sse  = sseL + sseR\n",
    "                if sse < best[4]:\n",
    "                    best = (j, t, L, R, sse)\n",
    "\n",
    "        return best[0], best[1], best[2], best[3]\n",
    "\n",
    "    # ----- internal: predict one -----\n",
    "    def _traverse(self, x, node: Node) -> float:\n",
    "        while node.value is None:\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n",
    "\n",
    "# ---- subtree stat maintenance (no indices stored) ----\n",
    "def _update_subtree_stats(node: Node) -> Tuple[float, int]:\n",
    "    if node.value is not None:\n",
    "        node.subtree_sse = leaf_sse_from_stats(node.sw, node.swy, node.swy2)\n",
    "        node.subtree_leaves = 1\n",
    "        return node.subtree_sse, 1\n",
    "    sL, lL = _update_subtree_stats(node.left)\n",
    "    sR, lR = _update_subtree_stats(node.right)\n",
    "    node.subtree_sse = sL + sR\n",
    "    node.subtree_leaves = lL + lR\n",
    "    return node.subtree_sse, node.subtree_leaves\n",
    "\n",
    "def _alpha_of(node: Node) -> float:\n",
    "    if node.value is not None:\n",
    "        return np.inf\n",
    "    sse_leaf = leaf_sse_from_stats(node.sw, node.swy, node.swy2)\n",
    "    denom = max(node.subtree_leaves - 1, 1e-12)\n",
    "    return (sse_leaf - node.subtree_sse) / denom\n",
    "\n",
    "def _collect_internal(node: Node, acc: List[Node]):\n",
    "    if node is None or node.value is not None:\n",
    "        return\n",
    "    acc.append(node)\n",
    "    _collect_internal(node.left, acc)\n",
    "    _collect_internal(node.right, acc)\n",
    "\n",
    "def _prune_once_inplace(root: Node) -> float:\n",
    "    \"\"\"Prune all nodes with minimal α (weakest-link). Returns α*.\"\"\"\n",
    "    _update_subtree_stats(root)\n",
    "    internals: List[Node] = []\n",
    "    _collect_internal(root, internals)\n",
    "    if not internals:\n",
    "        return np.inf\n",
    "    alphas = np.array([_alpha_of(n) for n in internals])\n",
    "    a_star = float(alphas.min())\n",
    "    tol = 1e-12\n",
    "\n",
    "    def prune_mark(n: Node):\n",
    "        if n.value is not None:\n",
    "            return\n",
    "        a = _alpha_of(n)\n",
    "        if abs(a - a_star) <= tol:\n",
    "            # turn node into a LEAF using its fixed stats\n",
    "            mu = (n.swy / n.sw) if n.sw > 0 else 0.0\n",
    "            n.feature = n.threshold = None\n",
    "            n.left = n.right = None\n",
    "            n.value = float(mu)\n",
    "            n.subtree_sse = leaf_sse_from_stats(n.sw, n.swy, n.swy2)\n",
    "            n.subtree_leaves = 1\n",
    "        else:\n",
    "            prune_mark(n.left); prune_mark(n.right)\n",
    "\n",
    "    prune_mark(root)\n",
    "    _update_subtree_stats(root)\n",
    "    return a_star\n",
    "\n",
    "def pruning_path_stream(root: Node):\n",
    "    \"\"\"\n",
    "    Generator yielding (alpha_star, leaves_now) while pruning in place.\n",
    "    Starts from current tree; first yield is alpha to prune to next simpler tree.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        a = _prune_once_inplace(root)\n",
    "        yield a, root.subtree_leaves\n",
    "        if root.subtree_leaves == 1 or not np.isfinite(a):\n",
    "            break\n",
    "\n",
    "def predict_with_root(X: np.ndarray, root: Node) -> np.ndarray:\n",
    "    X = np.asarray(X, float)\n",
    "    def _one(x, node):\n",
    "        while node.value is None:\n",
    "            node = node.left if x[node.feature] <= node.threshold else node.right\n",
    "        return node.value\n",
    "    return np.array([_one(X[i], root) for i in range(X.shape[0])], float)\n",
    "\n",
    "def kfold_indices(n_samples: int, n_splits: int, seed=42, shuffle=True):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n_samples, dtype=int)\n",
    "    if shuffle: rng.shuffle(idx)\n",
    "    folds = np.array_split(idx, n_splits)\n",
    "    for i in range(n_splits):\n",
    "        val = folds[i]\n",
    "        tr  = np.concatenate([folds[j] for j in range(n_splits) if j != i])\n",
    "        yield tr, val\n",
    "\n",
    "def cv_select_alpha_memorylight(X, y, w, base_params, kfold=5, seed=7):\n",
    "    \"\"\"\n",
    "    Memory-light α selection:\n",
    "    1) Build base tree on ALL data; stream its pruning alphas into a global α grid.\n",
    "    2) For each fold, rebuild base tree on train; stream pruning; record (alpha, score) pairs.\n",
    "    3) For each global α, use the fold's first α' >= α (right-bin) score.\n",
    "    \"\"\"\n",
    "    # Global α grid\n",
    "    base_all = DecisionTreeRegressorScratch(**base_params).fit(X, y, w)\n",
    "    global_alphas = []\n",
    "    # make a fresh copy by re-fitting (cheaper than deep copy)\n",
    "    root_all = DecisionTreeRegressorScratch(**base_params).fit(X, y, w).root\n",
    "    for a, _ in pruning_path_stream(root_all):\n",
    "        if np.isfinite(a):\n",
    "            global_alphas.append(a)\n",
    "        if getattr(root_all, \"subtree_leaves\", 1) == 1:\n",
    "            break\n",
    "    global_alphas = np.array(global_alphas, float)\n",
    "    if global_alphas.size == 0:\n",
    "        return {\"alpha\": 0.0, \"cv_wmae\": wmae(y, predict_with_root(X, base_all.root), w),\n",
    "                \"alphas\": [], \"cv_curve\": np.array([0.0])}\n",
    "\n",
    "    # Per-fold streamed curves (α_folds, score_folds)\n",
    "    fold_alphas = []\n",
    "    fold_scores = []\n",
    "    for tr, va in kfold_indices(len(y), kfold, seed=seed):\n",
    "        base = DecisionTreeRegressorScratch(**base_params).fit(X[tr], y[tr], w[tr])\n",
    "        a_list = []\n",
    "        s_list = []\n",
    "        # evaluate current (unpruned) too: treat as alpha=0 (optional)\n",
    "        yhat0 = predict_with_root(X[va], base.root)\n",
    "        s_list.append(wmae(y[va], yhat0, w[va]))\n",
    "        a_list.append(0.0)\n",
    "        # stream pruning\n",
    "        for a, _ in pruning_path_stream(base.root):\n",
    "            if not np.isfinite(a):  # finished\n",
    "                break\n",
    "            yhat = predict_with_root(X[va], base.root)\n",
    "            s_list.append(wmae(y[va], yhat, w[va]))\n",
    "            a_list.append(a)\n",
    "            if getattr(base.root, \"subtree_leaves\", 1) == 1:\n",
    "                break\n",
    "        fold_alphas.append(np.array(a_list, float))\n",
    "        fold_scores.append(np.array(s_list, float))\n",
    "\n",
    "    # Aggregate on the global α grid by right-binning each fold's curve\n",
    "    mean_scores = []\n",
    "    for A in global_alphas:\n",
    "        acc = []\n",
    "        for a_arr, s_arr in zip(fold_alphas, fold_scores):\n",
    "            # pick first index where a_arr >= A; if none, use last\n",
    "            j = int(np.searchsorted(a_arr, A, side=\"left\"))\n",
    "            if j >= len(s_arr): j = len(s_arr) - 1\n",
    "            acc.append(s_arr[j])\n",
    "        mean_scores.append(np.mean(acc))\n",
    "    mean_scores = np.array(mean_scores, float)\n",
    "    kbest = int(np.argmin(mean_scores))\n",
    "    return {\"alpha\": float(global_alphas[kbest]),\n",
    "            \"cv_wmae\": float(mean_scores[kbest]),\n",
    "            \"alphas\": global_alphas,\n",
    "            \"cv_curve\": mean_scores}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-pruning (max_depth=11, min_leaf_weight=9.0): WMAE=0.182040\n",
      "Pre-pruning (max_depth=11, min_leaf_weight=11.0): WMAE=0.182022\n",
      "Pre-pruning (max_depth=11, min_leaf_weight=13.0): WMAE=0.181984\n",
      "Pre-pruning (max_depth=11, min_leaf_weight=15.0): WMAE=0.182097\n",
      "Pre-pruning (max_depth=11, min_leaf_weight=20.0): WMAE=0.181914\n",
      "Pre-pruning (max_depth=11, min_leaf_weight=50.0): WMAE=0.182112\n",
      "Pre-pruning (max_depth=13, min_leaf_weight=9.0): WMAE=0.181675\n",
      "Pre-pruning (max_depth=13, min_leaf_weight=11.0): WMAE=0.181740\n",
      "Pre-pruning (max_depth=13, min_leaf_weight=13.0): WMAE=0.181659\n",
      "Pre-pruning (max_depth=13, min_leaf_weight=15.0): WMAE=0.181849\n",
      "Pre-pruning (max_depth=13, min_leaf_weight=20.0): WMAE=0.181738\n",
      "Pre-pruning (max_depth=13, min_leaf_weight=50.0): WMAE=0.182081\n",
      "Pre-pruning (max_depth=15, min_leaf_weight=9.0): WMAE=0.181912\n",
      "Pre-pruning (max_depth=15, min_leaf_weight=11.0): WMAE=0.181755\n",
      "Pre-pruning (max_depth=15, min_leaf_weight=13.0): WMAE=0.181817\n",
      "Pre-pruning (max_depth=15, min_leaf_weight=15.0): WMAE=0.181826\n",
      "Pre-pruning (max_depth=15, min_leaf_weight=20.0): WMAE=0.181777\n",
      "Pre-pruning (max_depth=15, min_leaf_weight=50.0): WMAE=0.182019\n",
      "Pre-pruning (max_depth=17, min_leaf_weight=9.0): WMAE=0.181921\n",
      "Pre-pruning (max_depth=17, min_leaf_weight=11.0): WMAE=0.181552\n",
      "Pre-pruning (max_depth=17, min_leaf_weight=13.0): WMAE=0.181692\n",
      "Pre-pruning (max_depth=17, min_leaf_weight=15.0): WMAE=0.181927\n",
      "Pre-pruning (max_depth=17, min_leaf_weight=20.0): WMAE=0.181797\n",
      "Pre-pruning (max_depth=17, min_leaf_weight=50.0): WMAE=0.182118\n",
      "Pre-pruning (max_depth=19, min_leaf_weight=9.0): WMAE=0.182051\n",
      "Pre-pruning (max_depth=19, min_leaf_weight=11.0): WMAE=0.181683\n",
      "Pre-pruning (max_depth=19, min_leaf_weight=13.0): WMAE=0.181747\n",
      "Pre-pruning (max_depth=19, min_leaf_weight=15.0): WMAE=0.181848\n",
      "Pre-pruning (max_depth=19, min_leaf_weight=20.0): WMAE=0.181513\n",
      "Pre-pruning (max_depth=19, min_leaf_weight=50.0): WMAE=0.182029\n",
      "Pre-pruning (max_depth=21, min_leaf_weight=9.0): WMAE=0.182125\n",
      "Pre-pruning (max_depth=21, min_leaf_weight=11.0): WMAE=0.181747\n",
      "Pre-pruning (max_depth=21, min_leaf_weight=13.0): WMAE=0.181718\n",
      "Pre-pruning (max_depth=21, min_leaf_weight=15.0): WMAE=0.181878\n",
      "Pre-pruning (max_depth=21, min_leaf_weight=20.0): WMAE=0.181569\n",
      "Pre-pruning (max_depth=21, min_leaf_weight=50.0): WMAE=0.181947\n",
      "Chosen pre-pruning caps: {'max_depth': 19, 'min_leaf_weight': 20.0, 'wmae': 0.181512531939047}\n",
      "\n",
      "Validation metrics (chosen caps):\n",
      " WMAE         : 0.181512531939047\n",
      " WRMSE        : 0.7420426405355376\n",
      " Weighted R^2 : 0.00808899246315209\n",
      " Poisson Dev. : 62097.72179452211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_1972\\4195817464.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
      "C:\\Users\\Emil\\AppData\\Local\\Temp\\ipykernel_1972\\4195817464.py:30: RuntimeWarning: invalid value encountered in multiply\n",
      "  term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Load + preprocess + tiny pre-pruning CV\n",
    "# =============================\n",
    "import pandas as pd\n",
    "from preprocessing.preprocessing_utils import preprocess_for_tree  # your function\n",
    "\n",
    "# Load\n",
    "train = pd.read_csv(\"../data/claims_train.csv\")\n",
    "test  = pd.read_csv(\"../data/claims_test.csv\")\n",
    "\n",
    "# Preprocess (no scaling; OHE cats; returns rate & exposure)\n",
    "X_tr, y_tr_rate, w_tr = preprocess_for_tree(train)\n",
    "X_te, y_te_rate, w_te = preprocess_for_tree(test)\n",
    "\n",
    "# Reconstruct counts for Poisson deviance\n",
    "y_tr_cnt = (y_tr_rate * w_tr).to_numpy(float)\n",
    "y_te_cnt = (y_te_rate * w_te).to_numpy(float)\n",
    "\n",
    "# To NumPy (OPTIONAL: float32 to save memory)\n",
    "X_np = X_tr.values.astype(np.float32)\n",
    "y_np = y_tr_rate.values.astype(np.float32)\n",
    "w_np = w_tr.values.astype(np.float32)\n",
    "\n",
    "# Simple hold-out split for quick validation in the grid\n",
    "rng = np.random.default_rng(42)\n",
    "idx = np.arange(len(y_np))\n",
    "rng.shuffle(idx)\n",
    "cut = int(0.8 * len(idx))\n",
    "tr_idx, va_idx = idx[:cut], idx[cut:]\n",
    "\n",
    "X_tr_np, y_tr_np, w_tr_np = X_np[tr_idx], y_np[tr_idx], w_np[tr_idx]\n",
    "X_va_np, y_va_np, w_va_np = X_np[va_idx], y_np[va_idx], w_np[va_idx]\n",
    "y_va_cnt_np = y_tr_cnt[va_idx]  # counts for Poisson deviance\n",
    "\n",
    "# ---- Tiny grid over pre-pruning caps (selection by WMAE) ----\n",
    "depth_grid = [5,10,25,50,100]\n",
    "leafw_grid = [10,15,20,25,30,50]\n",
    "\n",
    "best = None\n",
    "for d in depth_grid:\n",
    "    for m in leafw_grid:\n",
    "        tree = DecisionTreeRegressorScratch(max_depth=d, min_leaf_weight=m).fit(X_tr_np, y_tr_np, w_tr_np)\n",
    "        yhat_va = tree.predict(X_va_np)\n",
    "        score = wmae(y_va_np, yhat_va, w_va_np)\n",
    "        if (best is None) or (score < best[\"wmae\"]):\n",
    "            best = {\"max_depth\": d, \"min_leaf_weight\": m, \"wmae\": score, \"yhat_va\": yhat_va}\n",
    "        print(f\"Pre-pruning (max_depth={d}, min_leaf_weight={m}): WMAE={score:.6f}\")\n",
    "\n",
    "print(\"Chosen pre-pruning caps:\", {k: best[k] for k in [\"max_depth\",\"min_leaf_weight\",\"wmae\"]})\n",
    "\n",
    "# Report validation metrics for the chosen caps\n",
    "yhat_va = best[\"yhat_va\"]\n",
    "print(\"\\nValidation metrics (chosen caps):\")\n",
    "print(\" WMAE         :\", wmae(y_va_np, yhat_va, w_va_np))\n",
    "print(\" WRMSE        :\", wrmse(y_va_np, yhat_va, w_va_np))\n",
    "print(\" Weighted R^2 :\", weighted_r2(y_va_np, yhat_va, w_va_np))\n",
    "print(\" Poisson Dev. :\", poisson_deviance(y_va_cnt_np, w_va_np, yhat_va))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m base_params = {\u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m: best[\u001b[33m\"\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmin_leaf_weight\u001b[39m\u001b[33m\"\u001b[39m: best[\u001b[33m\"\u001b[39m\u001b[33mmin_leaf_weight\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Select alpha with memory-light CV\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m alpha_sel = \u001b[43mcv_select_alpha_memorylight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkfold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m alpha_star = alpha_sel[\u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mChosen α:\u001b[39m\u001b[33m\"\u001b[39m, alpha_star, \u001b[33m\"\u001b[39m\u001b[33m| CV WMAE:\u001b[39m\u001b[33m\"\u001b[39m, alpha_sel[\u001b[33m\"\u001b[39m\u001b[33mcv_wmae\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 276\u001b[39m, in \u001b[36mcv_select_alpha_memorylight\u001b[39m\u001b[34m(X, y, w, base_params, kfold, seed)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(a):  \u001b[38;5;66;03m# finished\u001b[39;00m\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m yhat = \u001b[43mpredict_with_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mva\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m s_list.append(wmae(y[va], yhat, w[va]))\n\u001b[32m    278\u001b[39m a_list.append(a)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 221\u001b[39m, in \u001b[36mpredict_with_root\u001b[39m\u001b[34m(X, root)\u001b[39m\n\u001b[32m    218\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m root.subtree_leaves == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(a):\n\u001b[32m    219\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_with_root\u001b[39m(X: np.ndarray, root: Node) -> np.ndarray:\n\u001b[32m    222\u001b[39m     X = np.asarray(X, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_one\u001b[39m(x, node):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Post-pruning via streaming CCP (α) + final eval\n",
    "# =============================\n",
    "base_params = {\"max_depth\": best[\"max_depth\"], \"min_leaf_weight\": best[\"min_leaf_weight\"]}\n",
    "\n",
    "# Select alpha with memory-light CV\n",
    "alpha_sel = cv_select_alpha_memorylight(X_np, y_np, w_np, base_params=base_params, kfold=5, seed=7)\n",
    "alpha_star = alpha_sel[\"alpha\"]\n",
    "print(\"Chosen α:\", alpha_star, \"| CV WMAE:\", alpha_sel[\"cv_wmae\"])\n",
    "\n",
    "# Fit base tree on ALL training with chosen caps\n",
    "final_base = DecisionTreeRegressorScratch(**base_params).fit(X_np, y_np, w_np)\n",
    "\n",
    "# Prune IN PLACE until we reach α >= alpha_star (streaming; no tree copies)\n",
    "# (We advance pruning steps and stop once the just-applied step's α ≥ alpha_star.)\n",
    "alpha_reached = 0.0\n",
    "for a, _ in pruning_path_stream(final_base.root):\n",
    "    alpha_reached = a\n",
    "    if not np.isfinite(a) or a >= alpha_star or getattr(final_base.root, \"subtree_leaves\", 1) == 1:\n",
    "        break\n",
    "    print(f\"Pruned to α={a}, leaves={final_base.root.subtree_leaves}\")\n",
    "\n",
    "# ---- Test evaluation ----\n",
    "X_te_np = X_te.values.astype(np.float32)\n",
    "y_te_np = y_te_rate.values.astype(np.float32)\n",
    "w_te_np = w_te.values.astype(np.float32)\n",
    "yhat_te = predict_with_root(X_te_np, final_base.root)\n",
    "\n",
    "print(\"\\nTest metrics (final pruned tree):\")\n",
    "print(\" WMAE         :\", wmae(y_te_np, yhat_te, w_te_np))\n",
    "print(\" WRMSE        :\", wrmse(y_te_np, yhat_te, w_te_np))\n",
    "print(\" Weighted R^2 :\", weighted_r2(y_te_np, yhat_te, w_te_np))\n",
    "y_te_cnt_np = (y_te_np * w_te_np).astype(np.float32)\n",
    "print(\" Poisson Dev. :\", poisson_deviance(y_te_cnt_np, w_te_np, yhat_te))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
