{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Metrics (primary = WMAE) ----------\n",
    "def wmae(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return (np.abs(y - yhat) * w).sum() / w.sum()\n",
    "\n",
    "def wrmse(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    return np.sqrt(((y - yhat)**2 * w).sum() / w.sum())\n",
    "\n",
    "def weighted_r2(y, yhat, w):\n",
    "    w = np.asarray(w, float)\n",
    "    ybar = (y * w).sum() / w.sum()\n",
    "    ss_res = ((y - yhat)**2 * w).sum()\n",
    "    ss_tot = ((y - ybar)**2 * w).sum()\n",
    "    return 1.0 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "def poisson_deviance(y_counts, exposure, yhat_rate):\n",
    "    y = np.asarray(y_counts, float)\n",
    "    lam = np.clip(yhat_rate, 1e-12, None) * np.asarray(exposure, float)\n",
    "    term = np.where(y > 0, y * np.log(y / lam), 0.0) - (y - lam)\n",
    "    return 2.0 * term.sum()\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "def wmean(y, w):\n",
    "    w = np.asarray(w, float)\n",
    "    sw = w.sum()\n",
    "    return (y * w).sum() / sw if sw > 0 else 0.0\n",
    "\n",
    "def leaf_sse(y, w):\n",
    "    mu = wmean(y, w)\n",
    "    return ((y - mu)**2 * w).sum()\n",
    "\n",
    "# ---------- Node ----------\n",
    "@dataclass\n",
    "class Node:\n",
    "    feat: Optional[int]\n",
    "    thr: Optional[float]\n",
    "    left: Optional[\"Node\"]\n",
    "    right: Optional[\"Node\"]\n",
    "    pred: float\n",
    "    idx: np.ndarray       # indices of samples at this node\n",
    "    sse: float = 0.0      # subtree SSE (sum of leaf SSEs)\n",
    "    leaves: int = 1       # number of leaves in subtree\n",
    "\n",
    "# ---------- Split search ----------\n",
    "def best_split(X, y, w, idx, min_leaf_w):\n",
    "    \"\"\"\n",
    "    Scan all features and candidate thresholds (midpoints).\n",
    "    Works for numeric and one-hot columns (0/1 => threshold=0.5).\n",
    "    Returns (j, t, L_idx, R_idx, child_sse) or None.\n",
    "    \"\"\"\n",
    "    if idx.size <= 1:\n",
    "        return None\n",
    "    yb = y[idx]\n",
    "    if np.allclose(yb, yb[0]):\n",
    "        return None\n",
    "\n",
    "    best = None\n",
    "    n, d = X.shape\n",
    "    for j in range(d):\n",
    "        xj = X[idx, j]\n",
    "        uniq = np.unique(xj)\n",
    "        if uniq.size <= 1:\n",
    "            continue\n",
    "        if uniq.size == 2 and uniq.min() == 0.0 and uniq.max() == 1.0:\n",
    "            candidates = [0.5]                      # one-hot split\n",
    "        else:\n",
    "            u = np.unique(np.sort(xj))\n",
    "            candidates = (u[:-1] + u[1:]) / 2.0     # midpoints\n",
    "\n",
    "        for t in candidates:\n",
    "            L_mask = xj < t\n",
    "            if not L_mask.any() or L_mask.all():\n",
    "                continue\n",
    "            L = idx[L_mask]\n",
    "            R = idx[~L_mask]\n",
    "            # exposure-weighted min leaf size\n",
    "            if w[L].sum() < min_leaf_w or w[R].sum() < min_leaf_w:\n",
    "                continue\n",
    "\n",
    "            sseL = leaf_sse(y[L], w[L])\n",
    "            sseR = leaf_sse(y[R], w[R])\n",
    "            sse_total = sseL + sseR\n",
    "            if (best is None) or (sse_total < best[4]):\n",
    "                best = (j, t, L, R, sse_total)\n",
    "    return best\n",
    "\n",
    "# ---------- Build tree (pre-pruned) ----------\n",
    "def build_tree(X, y, w, idx=None, max_depth=6, min_leaf_w=10.0, depth=0):\n",
    "    if idx is None:\n",
    "        idx = np.arange(X.shape[0], dtype=int)\n",
    "\n",
    "    pred = wmean(y[idx], w[idx])\n",
    "\n",
    "    # stop rules (simple & readable)\n",
    "    if depth >= max_depth or w[idx].sum() < 2 * min_leaf_w or np.allclose(y[idx], y[idx][0]):\n",
    "        leaf = Node(None, None, None, None, pred, idx.copy())\n",
    "        leaf.sse = leaf_sse(y[idx], w[idx])\n",
    "        leaf.leaves = 1\n",
    "        return leaf\n",
    "\n",
    "    split = best_split(X, y, w, idx, min_leaf_w)\n",
    "    if split is None:\n",
    "        leaf = Node(None, None, None, None, pred, idx.copy())\n",
    "        leaf.sse = leaf_sse(y[idx], w[idx])\n",
    "        leaf.leaves = 1\n",
    "        return leaf\n",
    "\n",
    "    j, t, L, R, _ = split\n",
    "    left  = build_tree(X, y, w, L, max_depth, min_leaf_w, depth+1)\n",
    "    right = build_tree(X, y, w, R, max_depth, min_leaf_w, depth+1)\n",
    "\n",
    "    node = Node(j, t, left, right, pred, idx.copy())\n",
    "    node.sse    = left.sse + right.sse\n",
    "    node.leaves = left.leaves + right.leaves\n",
    "    return node\n",
    "\n",
    "# ---------- Predict ----------\n",
    "def predict_one(x, node: Node):\n",
    "    while node.feat is not None:\n",
    "        node = node.left if x[node.feat] < node.thr else node.right\n",
    "    return node.pred\n",
    "\n",
    "def predict_tree(X, root: Node):\n",
    "    return np.array([predict_one(X[i], root) for i in range(X.shape[0])], float)\n",
    "\n",
    "# ---------- Weakest-link pruning to a target #leaves ----------\n",
    "def _compute_stats(node: Node, X, y, w):\n",
    "    \"\"\"Recompute subtree SSE and leaf counts bottom-up (use when structure changed).\"\"\"\n",
    "    if node.feat is None:\n",
    "        node.sse = leaf_sse(y[node.idx], w[node.idx])\n",
    "        node.leaves = 1\n",
    "        return node.sse, node.leaves\n",
    "    sL, lL = _compute_stats(node.left, X, y, w)\n",
    "    sR, lR = _compute_stats(node.right, X, y, w)\n",
    "    node.sse = sL + sR\n",
    "    node.leaves = lL + lR\n",
    "    return node.sse, node.leaves\n",
    "\n",
    "def _alpha(node: Node, y, w):\n",
    "    \"\"\"\n",
    "    α_t = (SSE_if_pruned_to_leaf - SSE_subtree) / (leaves_subtree - 1)\n",
    "    (∞ for leaves)\n",
    "    \"\"\"\n",
    "    if node.feat is None:\n",
    "        return np.inf\n",
    "    sse_leaf = leaf_sse(y[node.idx], w[node.idx])\n",
    "    denom = max(node.leaves - 1, 1e-12)\n",
    "    return (sse_leaf - node.sse) / denom\n",
    "\n",
    "def _collect_internal(node: Node) -> List[Node]:\n",
    "    if node is None or node.feat is None:\n",
    "        return []\n",
    "    return [node] + _collect_internal(node.left) + _collect_internal(node.right)\n",
    "\n",
    "def prune_to_leaves(root: Node, target_leaves: int, X, y, w):\n",
    "    \"\"\"\n",
    "    Greedy weakest-link pruning: repeatedly prune the node with smallest α\n",
    "    until the tree has <= target_leaves.\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    root = copy.deepcopy(root)\n",
    "    _compute_stats(root, X, y, w)\n",
    "\n",
    "    while root.leaves > target_leaves:\n",
    "        nodes = _collect_internal(root)\n",
    "        if not nodes:\n",
    "            break\n",
    "        alphas = np.array([_alpha(n, y, w) for n in nodes])\n",
    "        k = int(np.argmin(alphas))\n",
    "        victim = nodes[k]\n",
    "\n",
    "        # make victim a leaf\n",
    "        victim.feat = None\n",
    "        victim.thr = None\n",
    "        victim.left = None\n",
    "        victim.right = None\n",
    "        victim.pred = wmean(y[victim.idx], w[victim.idx])\n",
    "        victim.sse = leaf_sse(y[victim.idx], w[victim.idx])\n",
    "        victim.leaves = 1\n",
    "\n",
    "        _compute_stats(root, X, y, w)\n",
    "\n",
    "    return root\n",
    "\n",
    "# ---------- Simple K-fold (no sklearn) ----------\n",
    "def kfold_indices(n_samples: int, n_splits: int, shuffle=True, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n_samples, dtype=int)\n",
    "    if shuffle:\n",
    "        rng.shuffle(idx)\n",
    "    folds = np.array_split(idx, n_splits)\n",
    "    for i in range(n_splits):\n",
    "        val_idx = folds[i]\n",
    "        train_idx = np.concatenate([folds[j] for j in range(n_splits) if j != i])\n",
    "        yield train_idx, val_idx\n",
    "\n",
    "# ---------- CV #1: tune (max_depth, min_leaf_w) ----------\n",
    "def cv_preprune(X, y, w, depths=(3,5,7,9), min_leafs=(5.0,10.0,20.0), folds=5):\n",
    "    best = None\n",
    "    for d in depths:\n",
    "        for m in min_leafs:\n",
    "            scores = []\n",
    "            for tr, va in kfold_indices(len(y), folds, shuffle=True, seed=42):\n",
    "                root = build_tree(X, y, w, idx=tr, max_depth=d, min_leaf_w=m)\n",
    "                yhat = predict_tree(X[va], root)\n",
    "                scores.append(wmae(y[va], yhat, w[va]))\n",
    "            mean_score = float(np.mean(scores))\n",
    "            if (best is None) or (mean_score < best[\"wmae\"]):\n",
    "                best = {\"max_depth\": d, \"min_leaf_w\": m, \"wmae\": mean_score}\n",
    "    return best\n",
    "\n",
    "# ---------- CV #2: choose post-pruning by target #leaves ----------\n",
    "def count_leaves(node: Node) -> int:\n",
    "    if node.feat is None: return 1\n",
    "    return count_leaves(node.left) + count_leaves(node.right)\n",
    "\n",
    "def cv_prune_leaves(X, y, w, base_params, candidate_leaves=(2,3,4,6,8,12), folds=5):\n",
    "    best = None\n",
    "    for L in candidate_leaves:\n",
    "        scores = []\n",
    "        for tr, va in kfold_indices(len(y), folds, shuffle=True, seed=123):\n",
    "            root0 = build_tree(X, y, w, idx=tr,\n",
    "                               max_depth=base_params[\"max_depth\"],\n",
    "                               min_leaf_w=base_params[\"min_leaf_w\"])\n",
    "            # if tree already smaller than L, pruning does nothing\n",
    "            rootL = prune_to_leaves(root0, target_leaves=L, X=X, y=y, w=w)\n",
    "            yhat = predict_tree(X[va], rootL)\n",
    "            scores.append(wmae(y[va], yhat, w[va]))\n",
    "        mean_score = float(np.mean(scores))\n",
    "        if (best is None) or (mean_score < best[\"wmae\"]):\n",
    "            best = {\"max_leaves\": L, \"wmae\": mean_score}\n",
    "    return best\n",
    "\n",
    "# ---------- Fit final tree ----------\n",
    "def fit_final_tree(X, y, w, depths=(3,5,7,9), min_leafs=(5.0,10.0,20.0), leaves=(2,3,4,6,8,12), folds=5):\n",
    "    # CV #1: pick pre-pruning caps\n",
    "    pre = cv_preprune(X, y, w, depths=depths, min_leafs=min_leafs, folds=folds)\n",
    "    # Grow once on all data with best pre-pruning\n",
    "    root0 = build_tree(X, y, w, max_depth=pre[\"max_depth\"], min_leaf_w=pre[\"min_leaf_w\"])\n",
    "    # CV #2: pick pruning level (#leaves)\n",
    "    pr = cv_prune_leaves(X, y, w, pre, candidate_leaves=leaves, folds=folds)\n",
    "    # Prune final tree to best #leaves\n",
    "    final_root = prune_to_leaves(root0, target_leaves=pr[\"max_leaves\"], X=X, y=y, w=w)\n",
    "    return final_root, pre, pr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66df315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From your preprocessing:\n",
    "# X: pandas DataFrame (one-hot features), y_rate: Series (ClaimNb/Exposure), w_expo: Series (Exposure)\n",
    "X_np = X_tr.values.astype(float)\n",
    "y_np = y_tr.values.astype(float)\n",
    "w_np = w_tr.values.astype(float)\n",
    "\n",
    "# Fit final manual tree (you can tweak the small grids)\n",
    "final_tree, pre_caps, post_caps = fit_final_tree(\n",
    "    X_np, y_np, w_np,\n",
    "    depths=(5,7,9),\n",
    "    min_leafs=(5.0,10.0,20.0),\n",
    "    leaves=(2,3,4,6,8,12),\n",
    "    folds=5\n",
    ")\n",
    "print(\"Pre-pruning picked:\", pre_caps)\n",
    "print(\"Post-pruning picked:\", post_caps)\n",
    "\n",
    "# Predict on validation/test numpy arrays:\n",
    "# yhat_val = predict_tree(X_val_np, final_tree)\n",
    "# Evaluate (primary) WMAE; also report others:\n",
    "# print(\"WMAE :\", wmae(y_val_np, yhat_val, w_val_np))\n",
    "# print(\"WRMSE:\", wrmse(y_val_np, yhat_val, w_val_np))\n",
    "# print(\"R2_w :\", weighted_r2(y_val_np, yhat_val, w_val_np))\n",
    "# print(\"Poisson dev.:\", poisson_deviance(ClaimNb_val, w_val_np, yhat_val))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
